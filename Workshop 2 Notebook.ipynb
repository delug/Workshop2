{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Mathematics Behind Data Science\n",
    "\n",
    "This workshop will cover material ranging from what a vector is all the way to L<sup>p</sup> norms, loss functions, and gradient descent! We want to emphasize that a strong math background is not required for this workshop, as we'll be presenting the material in a beginner oriented, hands-on way. That means that we will introduce material both in terms of what you may code up in any given project, and the abstract math objects which represent them. In the simplest case, a vector can be described as a 1D array, but that's not enough to justify many of the techniques employed in DL. In order to extend that, we will dive into the math that powers the code. Specifically, this workshop aims to cover:\n",
    "\n",
    "- Tensors\n",
    "    - Vectors\n",
    "    - Matrices\n",
    "    - Tensors (multi-dimensional arrays)\n",
    "\n",
    "- Functional Operations\n",
    "    - Sum/Product reductions\n",
    "    - Tensor tiling\n",
    "- Norms\n",
    "    - Euclidean distance\n",
    "    - Metrics\n",
    "    - L<sup>1</sup>, L<sup>2</sup>, L<sup>p</sup> norms\n",
    "- Loss Functions\n",
    "    - Mean Squared Error\n",
    "    - Cross Entropy\n",
    "- Dimensionality Reduction\n",
    "    - Principle Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and Data\n",
    "\n",
    "### Vectors\n",
    "\n",
    "Vectors for the basis for the vast majority of data-science but are especially significant in deep learning! To begin with, vectors are the most fundamental way by which we represent data. Ultimately, a vector can be considered a \"list\" of numbers, with each number being assigned an **index**. These indices give us a way to explicitly refer to the individual elements composing the vector. Extending past that, we'll also talk about the operations that can occur using vectors, and more complex ways to manipulate them. Python doesn't come with any data type that's particularly nice for what we intend to do, so instead we look to Numpy's arrays. Numpy provides us an incredibly convenient way to represent such an abstract, and we'll explore it in great depth throughout this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[5 6 7]\n",
      "[1 2 3 4]\n",
      "\n",
      "1\n",
      "6\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True) # Convenience setting. Can safely ignore\n",
    "\n",
    "\n",
    "a=[1,2,3,4] # Standard Python list\n",
    "b=np.array([5,6,7]) # Numpy array made based on the given python list\n",
    "c=np.array(a) # Another way to create a numpy array based on an existing list\n",
    "\n",
    "print(a) \n",
    "print(b) # Note the numpy array prints without commas \",\"\n",
    "print(c) # This cleans up the print statement and is helpful for larger constructions\n",
    "\n",
    "print()\n",
    "print(a[0])\n",
    "print(b[1]) # Access elements just as you regularly would\n",
    "print(c[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, a vector is referred to as $N$ *dimensional* if and only if it has exactly $N$ *elements*. A vector is also a *tensor of order 1*, or alternatively, a *first order tensor* (we'll use the first name scheme). We're going to draw a sharp distinction between dimensionality and order. In a lot of data science literature, these two are generally interchangably referred to as *dimension*, but that leads to way too many headaches for us to justify using it. We've defined dimensionality for our purposes, so what is order? Well, roughly speaking the order of a vector/matrix/tensor is **the number of indices required to specify which element you're talking about**. So again, a vector is a first order tensor, meaning we need only one index to exactly specify which element we want to access. This is shown in our last example! \n",
    "\n",
    "For notation, we'll write out a vector, such as \"a\" in the code above like this: $\\vec a = \\left<1,2,3,4 \\right>$. Other acceptable notations include: $a=[1,2,3,4]^\\intercal$, or $a=\\begin{bmatrix}1\\\\2\\\\3\\\\4\\end{bmatrix}$. We use the first one because it's honestly just easier, and doesn't mess up vertical spacing unlike *some* methods.\n",
    "\n",
    "Now, suppose we have $\\vec{v}$, an $N$ dimensional vector, and suppose every element will be a real number, then we say that $\\vec{v}\\in\\mathbb{R}^N$ read as *v is an element of R-N* (not *R to the power of N*). If your elements are instead integers, the statement would be $\\vec{v}\\in\\mathbb{Z}^N$. In general, if your elements belong to a set $S$, then you write $\\vec{v}\\in S^N$\n",
    "\n",
    "So if we had a vector $\\vec{u}$ which had 6 elements which are real numbers, we'd write $\\vec{u}\\in\\mathbb{R}^6$. If it had 6 integers instead, $\\vec{u}\\in\\mathbb{Z}^6$. Now suppose 4 of those elements were real numbers, and 2 were integers, then $\\vec{u}\\in\\mathbb{R}^4\\times\\mathbb{Z}^2$. Note that $\\mathbb{R}^6=\\mathbb{R}^3\\times\\mathbb{R}^3$. This language gives us much more flexibility in discussing more complex ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** for convenience, we will stop writing the arrow above the vector, since in context it is generally clear whether or not a variable refers to a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices\n",
    "\n",
    "Many of you may be familiar with matrices, but most of you will have only seen them in passing. A matrix can be defined in many ways, but the most programmer-friendly description is as a **collection of elements**, much like how we defined our vectors! With matrices, however, we need two indices in order to specify which element we're talking about.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "1\n",
      "2\n",
      "4\n",
      "9\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([[1,2,3]\n",
    "          ,[4,5,6]\n",
    "          ,[7,8,9]]) # We'll explain this creation in a little bit\n",
    "\n",
    "print(x)\n",
    "print(x[0,0]) # First row, first column, the top-left element\n",
    "print(x[0,1]) # First row, second column\n",
    "print(x[1,0]) # Second row, first column\n",
    "print(x[2,2]) # Third row, third column, the bottom-right element\n",
    "\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above example, we require two indices (e.g 0,1) in order to specify a particular element, hence this is a *tensor of order 2*. So what exactly happened when we only specified one index? Well, it returned the first row of the matrix. This leads to another interpretation of a matrix: **a list of vectors**. If you have vectors $v_1,v_2,v_3$, then you can construct a matrix $A=[v_1,v_2,v_3]^\\intercal$ where our three vectors serve as *rows* in the matrix. In the above code, $v_1=\\left<1,2,3\\right>$, $v_2=\\left<4,5,6\\right>$, $v_3=\\left<7,8,9\\right>$. This is why when we run `print(x[0])` we get $v_1$. \n",
    "\n",
    "In math literature we generally construct matrices using vectors as their *columns*, and this can cause a bit of confusion. In practice it doesn't matter all too much though, since you can always use a handy function to change your rows to columns and vice versa known as the *transpose*, usually denoted as $A^\\intercal$. In fact, when we defined $A$ originally, we used the transpose so that we could set each $v_i$ as a row instead of column. Let's see it in action below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "[1 2 3]\n",
      "\n",
      "[[1 4 7]\n",
      " [2 5 8]\n",
      " [3 6 9]]\n",
      "\n",
      "[1 4 7]\n"
     ]
    }
   ],
   "source": [
    "print(x) # Regular\n",
    "print()\n",
    "print(x[0]) # First row of x\n",
    "\n",
    "print()\n",
    "y=x.transpose()\n",
    "print(y) # Switch rows with columns\n",
    "print()\n",
    "print(y[0]) # First row of y, but first column of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undeniably the most common operation regarding matrices is *matrix multiplication*. This can be a little unintuitive at first, but we hope to help explain the weird behaviour. First off, matrices need to have the right kind of shape before being multiplied together. We say a matrix $A$ is $m\\times n$ when it has $m$ rows and $n$ columns. If we want to multiply matrices $A$ and $B$ to get $C=AB$, then $B$ needs to have the same number of rows as $A$ has columns. So if $A$ is $m\\times n$ then $B$ must be $n\\times l$. Then $C$ would end up being an $m\\times l$ matrix. An easy, albeit mathematically lax, way to figure out shape compatibilities for matrices is that the shape of $C=AB$ is $(m\\times n)\\times(n\\times l)\\implies m\\times (n\\times n) \\times l \\implies m\\times l$. You can imagine it as the inner variable $n$ *canceling out*. Which also helps to remind you that $B$ needs to have the same number of rows as $A$ has columns, else the inner dimensions wouldn't be the same and hence wouldn't cancel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first matrix:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [1 4]]\n",
      "\n",
      "This is the second matrix:\n",
      "[[5 6]\n",
      " [7 8]]\n",
      "\n",
      "This is their product:\n",
      "[[19 22]\n",
      " [43 50]\n",
      " [33 38]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2],[3,4],[1,4]]) # A is 3x2\n",
    "B = np.array([[5,6],[7,8]]) # B is 2x2\n",
    "\n",
    "print('This is the first matrix:')\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "print('This is the second matrix:')\n",
    "print(B)\n",
    "print()\n",
    "\n",
    "print('This is their product:')\n",
    "\n",
    "#multiplies the 2 matrices together\n",
    "print(np.dot(A,B)) # AB is (3x2)x(2x2)=>3x(2x2)x2=>3x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what exactly is going on here? Let's start with a simple example using $A=[\\vec a_1,\\vec a_2]$ where $a_i\\in\\mathbb{R}^3$ (note since we're not using the transpose, each $\\vec a_i$ is a *column* of $A$), a $3x2$ matrix and $\\vec b= \\left<b_1,b_2 \\right>$, a $2x1$ matrix, which is alternatively just a *vector*, then $C=Ab$ is going to be $3\\times 1$, so it'll also be a vector! We can define $Ab=\\sum_{i=1}^2b_i\\vec a_i=b_1\\vec a_1 + b_2\\vec a_2$.\n",
    "\n",
    "**Note**: We switched back to arrow notation here since we'll be juggling between vectors and scalars, and so it's not entirely obvious in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 3x2 matrix A:\n",
      "[[1 1]\n",
      " [3 0]\n",
      " [2 1]]\n",
      "\n",
      "This a length 2 vector b:\n",
      "[-1  2]\n",
      "\n",
      "This is their product Ab:\n",
      "[ 1 -3  0]\n",
      "\n",
      "This is the first value of b times the first column of A:\n",
      "[-1 -3 -2]\n",
      "\n",
      "This is the second value of b times the second column of A:\n",
      "[2 0 2]\n",
      "\n",
      "This is their sum:\n",
      "[ 1 -3  0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initializes a 3x2 matrix A\n",
    "A = np.array([[1,1],[3,0],[2,1]])\n",
    "\n",
    "#initializes a vector b of length 2\n",
    "b = np.array([-1,2])\n",
    "\n",
    "print('This is a 3x2 matrix A:')\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "print('This a length 2 vector b:')\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "#multiplies Ab\n",
    "print('This is their product Ab:')\n",
    "print(np.dot(A,b))\n",
    "print()\n",
    "\n",
    "#demonstrates the internal steps in calculating Ab\n",
    "b1a1 = b[0] * np.transpose(A)[0]\n",
    "print('This is the first value of b times the first column of A:')\n",
    "print(b1a1)\n",
    "print()\n",
    "\n",
    "b2a2 = b[1] * np.transpose(A)[1]\n",
    "print('This is the second value of b times the second column of A:')\n",
    "print(b2a2)\n",
    "print()\n",
    "\n",
    "print('This is their sum:')\n",
    "print(b1a1 + b2a2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 5x4 matrix A:\n",
      "[[ 1  1  1  1]\n",
      " [ 1  0  1  0]\n",
      " [ 1  2  3  4]\n",
      " [ 0  0  0  0]\n",
      " [-1  1 -1  0]]\n",
      "\n",
      "Shape of A:  (5, 4)\n",
      "This is a length 4 vector b:\n",
      "[ 1  0 -1  2]\n",
      "\n",
      "This is the product Ab:\n",
      "[2 0 6 0 0]\n",
      "\n",
      "This is a 6x6 identity matrix I:\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "This is a length 6 vector v:\n",
      "[-1.  -0.6 -0.2  0.2  0.6  1. ]\n",
      "\n",
      "This is the product Iv:\n",
      "[-1.  -0.6 -0.2  0.2  0.6  1. ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initializes a 5x4 matrix A\n",
    "A = np.array([[1,1,1,1],[1,0,1,0],[1,2,3,4],[0,0,0,0],[-1,1,-1,0]])\n",
    "print('This is a 5x4 matrix A:')\n",
    "print(A)\n",
    "print()\n",
    "print(\"Shape of A: \",A.shape) # Prints the shape of A\n",
    "\n",
    "\n",
    "#initializes a vector b of length 4\n",
    "b = np.array([1,0,-1,2])\n",
    "print('This is a length 4 vector b:')\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "#multiplies Ab\n",
    "print('This is the product Ab:')\n",
    "print(np.dot(A,b))\n",
    "print()\n",
    "\n",
    "#initializes a 6x6 identity matrix\n",
    "I = np.eye(6)\n",
    "print('This is a 6x6 identity matrix I:')\n",
    "print(I)\n",
    "print()\n",
    "\n",
    "#initializes a vector v of length 6\n",
    "v = np.linspace(-1,1,6)\n",
    "print('This is a length 6 vector v:')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#multiplies Iv. Note the result of multiplication by the \"identity matrix\"\n",
    "print('This is the product Iv:')\n",
    "print(np.dot(I,v))\n",
    "print()\n",
    "\n",
    "#The next print statement would fail because it multiplies a 5x4 matrix A against a length 6 vector v\n",
    "#print(np.dot(A,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've only multiplied a matrix against a vector, but what about against another matrix? Well, once again, thinking about a matrix as a collection of vectors will help us out. Instead of getting the whole matrix in one shot, we'll do it column by column. Take the same $A$ as defined before, but now consider a matrix $B=[\\vec b_1, \\vec b_2, \\vec b_3]$ with $\\vec b_i\\in\\mathbb{R}^2$, so $B$ is a $2\\times3$, and since $A$ is a $3\\times2$, our resulting matrix $C=AB$ will be a $3\\times3$. We'll first calculate $\\vec c_1=\\sum_{i=1}^3b_{1,i}\\vec a_i=b_{1,1}\\vec a_1 + b_{1,2}\\vec a_2 +b_{1,3}\\vec a_3 $ where $b_{j,i}$ refers to the $i^\\text{th}$ element of vector $b_j$. Hopefully this looks at least a *little* familiar. This is the exact same process as we had for our matrix times vector case! So $\\vec c_i=A\\vec b_i$. There are many other ways to imagine matrix multiplication, each with their own merits, but this definition is what we recommend for most people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 2x3 matrix A:\n",
      "[[ 1  0  1]\n",
      " [-1  1  0]]\n",
      "\n",
      "This is a 3x2 matrix B:\n",
      "[[3 2]\n",
      " [4 1]\n",
      " [0 3]]\n",
      "\n",
      "This is the product AB:\n",
      "[[ 3  5]\n",
      " [ 1 -1]]\n",
      "\n",
      "This is the product A*b1:\n",
      "[3 1]\n",
      "\n",
      "This is the product A*b2:\n",
      "[ 5 -1]\n",
      "\n",
      "This is the matrix with columns A*b1 and A*b2:\n",
      "[[ 3  5]\n",
      " [ 1 -1]]\n",
      "\n",
      "This is the product BA:\n",
      "[[ 1  2  3]\n",
      " [ 3  1  4]\n",
      " [-3  3  0]]\n"
     ]
    }
   ],
   "source": [
    "#initializes a 2x3 matrix A\n",
    "A = np.array([[1,0,1],[-1,1,0]])\n",
    "print('This is a 2x3 matrix A:')\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "#initializes a 3x2 matrix B\n",
    "B = np.array([[3,2],[4,1],[0,3]])\n",
    "print('This is a 3x2 matrix B:')\n",
    "print(B)\n",
    "print()\n",
    "\n",
    "#multiplies AB\n",
    "print('This is the product AB:')\n",
    "print(np.dot(A,B))\n",
    "print()\n",
    "\n",
    "#multiplies A*b1\n",
    "Ab1 = np.dot(A,B[:,0])\n",
    "print('This is the product A*b1:')\n",
    "print(Ab1)\n",
    "print()\n",
    "\n",
    "#multiplies A*b2\n",
    "Ab2 = np.dot(A,B[:,1])\n",
    "print('This is the product A*b2:')\n",
    "print(Ab2)\n",
    "print()\n",
    "\n",
    "#constructs [A*b1, A*b2]\n",
    "Ab12 = np.stack([Ab1,Ab2], 1)\n",
    "print('This is the matrix with columns A*b1 and A*b2:')\n",
    "print(Ab12)\n",
    "print()\n",
    "\n",
    "#multiplies BA\n",
    "print('This is the product BA:')\n",
    "print(np.dot(B,A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 5x4 matrix A:\n",
      "[[ 1  1  1  1]\n",
      " [ 1  0  1  0]\n",
      " [ 1  2  3  4]\n",
      " [ 0  0  0  0]\n",
      " [-1  1 -1  0]]\n",
      "\n",
      "This is a length 4 vector b:\n",
      "[ 1  0 -1  2]\n",
      "\n",
      "This is the product Ab:\n",
      "[2 0 6 0 0]\n",
      "\n",
      "This is a 6x6 identity matrix I:\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "This is a length 6 vector v:\n",
      "[-1.  -0.6 -0.2  0.2  0.6  1. ]\n",
      "\n",
      "This is the product Iv:\n",
      "[-1.  -0.6 -0.2  0.2  0.6  1. ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initializes a 5x4 matrix A\n",
    "A = np.array([[1,1,1,1],[1,0,1,0],[1,2,3,4],[0,0,0,0],[-1,1,-1,0]])\n",
    "print('This is a 5x4 matrix A:')\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "#initializes a vector b of length 4\n",
    "b = np.array([1,0,-1,2])\n",
    "print('This is a length 4 vector b:')\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "#multiplies Ab\n",
    "print('This is the product Ab:')\n",
    "print(np.dot(A,b))\n",
    "print()\n",
    "\n",
    "#initializes a 6x6 identity matrix\n",
    "I = np.eye(6)\n",
    "print('This is a 6x6 identity matrix I:')\n",
    "print(I)\n",
    "print()\n",
    "\n",
    "#initializes a vector v of length 6\n",
    "v = np.linspace(-1,1,6)\n",
    "print('This is a length 6 vector v:')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#multiplies Iv. Note the result of multiplication by the \"identity matrix\"\n",
    "print('This is the product Iv:')\n",
    "print(np.dot(I,v))\n",
    "print()\n",
    "\n",
    "#The next print statement would fail because it multiplies a 5x4 matrix A against a length 6 vector v\n",
    "#print(np.dot(A,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** a very special case of matrix multiplication is when given two vectors $a,b$, we calculate $c=b^\\intercal a=\\sum_{i=1}^na_ib_i$, which is perhaps better known as the dot product! So $c=b^\\intercal a=a\\cdot b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are the greatest abstraction of vectors and matrices we'll be coverinf during this workshop. Tensors are best discussed in terms of their *order*. As a reminder, we defined *order* as the number of indices required to specify a single element in the object. We've already covered tensors of order 1 and 2 so let's look at *tensors of order 3*. Just as matrices were considered **lists of vectors**, a *tensors of order 3* can be considered a **list of matrices**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrices:\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]]\n",
      "[[13 14]\n",
      " [15 16]\n",
      " [17 18]]\n",
      "[[19 20]\n",
      " [21 22]\n",
      " [23 24]]\n",
      "\n",
      "This is t:\n",
      "[[[ 1  2]\n",
      "  [ 3  4]\n",
      "  [ 5  6]]\n",
      "\n",
      " [[ 7  8]\n",
      "  [ 9 10]\n",
      "  [11 12]]\n",
      "\n",
      " [[13 14]\n",
      "  [15 16]\n",
      "  [17 18]]\n",
      "\n",
      " [[19 20]\n",
      "  [21 22]\n",
      "  [23 24]]]\n",
      "(4, 3, 2)\n",
      "\n",
      "These are some elements of t:\n",
      "1\n",
      "7\n",
      "3\n",
      "2\n",
      "24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initializes a 4x3x2 tensor with the values from 1 to 24\n",
    "A1=np.array([[1,2],[3,4],[5,6]])\n",
    "A2=np.array([[7,8],[9,10],[11,12]])\n",
    "A3=np.array([[13,14],[15,16],[17,18]])\n",
    "A4=np.array([[19,20],[21,22],[23,24]])\n",
    "print(\"The matrices:\")\n",
    "print(A1)\n",
    "print(A2)\n",
    "print(A3)\n",
    "print(A4)\n",
    "t = np.array([A1,A2,A3,A4])\n",
    "print('\\nThis is t:')\n",
    "print(t)\n",
    "\n",
    "#prints the shape of the tensor t\n",
    "#note all\n",
    "print(t.shape)\n",
    "print()\n",
    "\n",
    "#note that to access a specific element, an index must be specified for each axis\n",
    "print('These are some elements of t:')\n",
    "print(t[0,0,0]) #element located in the first position of each axis\n",
    "print(t[1,0,0]) #element located in the second position of first axis, first position of second and third axes\n",
    "print(t[0,1,0]) #element located in the first position of first axis, second position of second axis, first position of third axis\n",
    "print(t[0,0,1]) #element located in the first position of first and second axes, second position of third axis\n",
    "print(t[3,2,1]) #element located in the fourth position of first axis, third position of second axis, and second position of third axis\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no reason to stop there though. In general we have *tensors of order $n$* which can be seen as lists of *tensors of order $(n-1)$* (...or as matrices of *tensors of order $(n-2)$*, or as a *tensors of order 3* of *tensors of order $(n-3)$*). Let's look at a *tensor of order 5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a big tensor's shape and values\n",
      "(2, 2, 2, 2, 2)\n",
      "[[[[[ 1  2]\n",
      "    [ 3  4]]\n",
      "\n",
      "   [[ 5  6]\n",
      "    [ 7  8]]]\n",
      "\n",
      "\n",
      "  [[[ 9 10]\n",
      "    [11 12]]\n",
      "\n",
      "   [[13 14]\n",
      "    [15 16]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[17 18]\n",
      "    [19 20]]\n",
      "\n",
      "   [[21 22]\n",
      "    [23 24]]]\n",
      "\n",
      "\n",
      "  [[[25 26]\n",
      "    [27 28]]\n",
      "\n",
      "   [[29 30]\n",
      "    [31 32]]]]]\n"
     ]
    }
   ],
   "source": [
    "#initializes a 2x2x2x2x2 tensor with values from 1 to 32\n",
    "b = np.array([[[[[1,2],[3,4]],[[5,6],[7,8]]],[[[9,10],[11,12]],[[13,14],[15,16]]]],[[[[17,18],[19,20]],[[21,22],[23,24]]],[[[25,26],[27,28]],[[29,30],[31,32]]]]])\n",
    "print('This is a big tensor\\'s shape and values')\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a pain to write out every value of a tensor, surrounded by the appropriate brackets, so for certain cases there are very handy intialization tools for creating some commonly used tensors. As well as commonly used tensors, numpy allows us to specify what types of elements they will contain. We've been assuming all our elements are real numbers, or equivelantly, `float` values. In general they can be 32-bit integers (`int32`), 64-bit integers (`int64`), 32-bit floats (`float32`), 64-bit floats (`float64`), 128-bit complex numbers (`complex`), booleans (`bool`), objects (`object`), strings (`string_`), or unicode characters (`unicoded_`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a tensor of zeroes:\n",
      "[[[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]]\n",
      "\n",
      "This is a tensor of ones:\n",
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]]\n",
      "\n",
      "This is a tensor with a constant value in each position:\n",
      "[[[42 42 42 42 42]\n",
      "  [42 42 42 42 42]]\n",
      "\n",
      " [[42 42 42 42 42]\n",
      "  [42 42 42 42 42]]]\n",
      "\n",
      "This is a tensor with random values in each position:\n",
      "[[[0.42118609 0.97747345 0.91497386 0.0629408  0.77644124]\n",
      "  [0.63281608 0.33425122 0.28135532 0.1361897  0.85222988]]\n",
      "\n",
      " [[0.80931678 0.09038362 0.44470766 0.9199382  0.45210217]\n",
      "  [0.96078022 0.06880233 0.32282673 0.41682529 0.39713642]]]\n",
      "\n",
      "This is a tensor formed by stacking the others along a new axis:\n",
      "[[[[ 0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.          0.          0.        ]\n",
      "   [ 0.          0.          0.          0.          0.        ]]]\n",
      "\n",
      "\n",
      " [[[ 1.          1.          1.          1.          1.        ]\n",
      "   [ 1.          1.          1.          1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.          1.          1.        ]\n",
      "   [ 1.          1.          1.          1.          1.        ]]]\n",
      "\n",
      "\n",
      " [[[42.         42.         42.         42.         42.        ]\n",
      "   [42.         42.         42.         42.         42.        ]]\n",
      "\n",
      "  [[42.         42.         42.         42.         42.        ]\n",
      "   [42.         42.         42.         42.         42.        ]]]\n",
      "\n",
      "\n",
      " [[[ 0.42118609  0.97747345  0.91497386  0.0629408   0.77644124]\n",
      "   [ 0.63281608  0.33425122  0.28135532  0.1361897   0.85222988]]\n",
      "\n",
      "  [[ 0.80931678  0.09038362  0.44470766  0.9199382   0.45210217]\n",
      "   [ 0.96078022  0.06880233  0.32282673  0.41682529  0.39713642]]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initializes a 2x2x5 tensor with the boolean value 0 in each position\n",
    "z = np.zeros([2,2,5], dtype=bool)\n",
    "print('This is a tensor of boolean zeroes:')\n",
    "print(z)\n",
    "print()\n",
    "\n",
    "#initializes a 2x2x5 tensor with the 32-bit float value 1 in each position\n",
    "o = np.ones([2,2,5], dtype=np.float32)\n",
    "print('This is a tensor of 32-bit float ones:')\n",
    "print(o)\n",
    "print()\n",
    "\n",
    "#initializes a 2x2x5 tensor with the 64-bit integer value of c in each position\n",
    "c = 42\n",
    "f = np.full([2,2,5], c, np.int64)\n",
    "print('This is a tensor with a constant 64-bit integer value in each position:')\n",
    "print(f)\n",
    "print()\n",
    "\n",
    "#initializes a 2x2x5 tensor with random values in each position\n",
    "r = np.random.random([2,2,5])\n",
    "print('This is a tensor with random values in each position:')\n",
    "print(r)\n",
    "print()\n",
    "\n",
    "#constructs a tensor from other tensors\n",
    "b = np.stack([z,o,f,r])\n",
    "print('This is a tensor formed by stacking the others along a new axis:')\n",
    "print(b)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we'll take a look at the different ways that you can manipulate tensors, starting with simple arithmetic operations. They can be broadly divided into two categories: scalar, and tensor. Scalar arithmetic is just arithmetic that involves one tensor and a scalar. Tensor arithmetic happens between two tensors. Here are some basic scalar arithmetic operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is v:\n",
      "[[[2.  3.5]\n",
      "  [4.  2. ]]\n",
      "\n",
      " [[1.5 1. ]\n",
      "  [2.  1. ]]]\n",
      "\n",
      "This is v+2:\n",
      "[[[4.  5.5]\n",
      "  [6.  4. ]]\n",
      "\n",
      " [[3.5 3. ]\n",
      "  [4.  3. ]]]\n",
      "\n",
      "This is v-2:\n",
      "[[[ 0.   1.5]\n",
      "  [ 2.   0. ]]\n",
      "\n",
      " [[-0.5 -1. ]\n",
      "  [ 0.  -1. ]]]\n",
      "\n",
      "This is v*2:\n",
      "[[[4. 7.]\n",
      "  [8. 4.]]\n",
      "\n",
      " [[3. 2.]\n",
      "  [4. 2.]]]\n",
      "\n",
      "This is v/2: \n",
      "[[[1.   1.75]\n",
      "  [2.   1.  ]]\n",
      "\n",
      " [[0.75 0.5 ]\n",
      "  [1.   0.5 ]]]\n"
     ]
    }
   ],
   "source": [
    "#initializes a 2x2x2 tensor with specified values\n",
    "v = np.array([[[2,3.5],[4,2]],[[1.5,1],[2,1]]])\n",
    "print('This is v:')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#constructs a new 2x2x2 tensor formed by adding 2 to each of the entries in v\n",
    "vAdd = v + 2\n",
    "print('This is v+2:')\n",
    "print(vAdd)\n",
    "print()\n",
    "\n",
    "#constructs a new 2x2x2 tensor formed by subtracting 2 from each of the entries in v\n",
    "vSubtract = v - 2\n",
    "print('This is v-2:')\n",
    "print(vSubtract)\n",
    "print()\n",
    "\n",
    "#constructs a new 2x2x2 tensor formed by multiplying each of the entries by 2\n",
    "vMultiply = v * 2\n",
    "print('This is v*2:')\n",
    "print(vMultiply)\n",
    "print()\n",
    "\n",
    "#constructs a new 2x2x2 tensor formed by dividing each of the entries by 2\n",
    "vDivide = v / 2\n",
    "print('This is v/2: ')\n",
    "print(vDivide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is v: \n",
      "[[[2.  3.5]\n",
      "  [4.  2. ]]\n",
      "\n",
      " [[1.5 1. ]\n",
      "  [2.  1. ]]]\n",
      "\n",
      "This is v after adding 1\n",
      "[[[3.  4.5]\n",
      "  [5.  3. ]]\n",
      "\n",
      " [[2.5 2. ]\n",
      "  [3.  2. ]]]\n",
      "\n",
      "This is v after dividing by 2\n",
      "[[[1.5  2.25]\n",
      "  [2.5  1.5 ]]\n",
      "\n",
      " [[1.25 1.  ]\n",
      "  [1.5  1.  ]]]\n",
      "\n",
      "This is w, changed by the operations performed on v: \n",
      "[[[1.5  2.25]\n",
      "  [2.5  1.5 ]]\n",
      "\n",
      " [[1.25 1.  ]\n",
      "  [1.5  1.  ]]]\n",
      "\n",
      "This is x, unchanged by the operations performed on v: \n",
      "[[[2.  3.5]\n",
      "  [4.  2. ]]\n",
      "\n",
      " [[1.5 1. ]\n",
      "  [2.  1. ]]]\n"
     ]
    }
   ],
   "source": [
    "#assigns the name w to v, creating a shallow copy\n",
    "w = v\n",
    "\n",
    "#constructs a new tensor x with the values of v, creating a deep copy\n",
    "x = np.copy(v)\n",
    "\n",
    "print('This is v: ')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#adds 1 to each value in v\n",
    "v += 1\n",
    "print('This is v after adding 1')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#divides each value in v by 2\n",
    "v /= 2\n",
    "print('This is v after dividing by 2')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "print('This is w, changed by the operations performed on v: ')\n",
    "print(w)\n",
    "print()\n",
    "\n",
    "print('This is x, unchanged by the operations performed on v: ')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These occur as element-wise operations. We say an operation is element wise when instead of applying the operation to the tensors, we can directly apply it to their elements instead. For example, if we have a tensor $A$ and a scalar $k$ so that $C=A+k$ then we say that $c_i=a_i+k$, and so the operation is instead defined for every individual element. Tensor arithmetic can also be element wise. Suppose we have tensors $A,B$ and define $C=A*B$, then we can instead define $C$ element-wise like this: for every element $c_{i,j}=a_{i,j}*b_{i,j}$. \n",
    "\n",
    "**Note:** here we use $*$ to represent element-wise multiplication instead of proper tensor/matrix multiplication which is instead denoted by placing the matrices/tensors adjacent to eachother, e.g. $C=AB$ \n",
    "\n",
    "Below are some examples of tensor arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 2x3x2 tensor S:\n",
      "[[[ 1  0]\n",
      "  [-2  1]\n",
      "  [ 3  2]]\n",
      "\n",
      " [[-3  0]\n",
      "  [ 2  2]\n",
      "  [ 0 -1]]]\n",
      "\n",
      "This is a 2x3x2 tensor T:\n",
      "[[[-2  4]\n",
      "  [ 3  3]\n",
      "  [-1 -1]]\n",
      "\n",
      " [[ 1 -3]\n",
      "  [ 1  1]\n",
      "  [ 2  1]]]\n",
      "\n",
      "This is the sum of S and T:\n",
      "[[[-1  4]\n",
      "  [ 1  4]\n",
      "  [ 2  1]]\n",
      "\n",
      " [[-2 -3]\n",
      "  [ 3  3]\n",
      "  [ 2  0]]]\n",
      "\n",
      "This is the difference of S and T:\n",
      "[[[ 3 -4]\n",
      "  [-5 -2]\n",
      "  [ 4  3]]\n",
      "\n",
      " [[-4  3]\n",
      "  [ 1  1]\n",
      "  [-2 -2]]]\n",
      "\n",
      "This is the element-wise product of S and T:\n",
      "[[[-2  0]\n",
      "  [-6  3]\n",
      "  [-3 -2]]\n",
      "\n",
      " [[-3  0]\n",
      "  [ 2  2]\n",
      "  [ 0 -1]]]\n",
      "\n",
      "This is the element-wise quotient of S and T:\n",
      "[[[-0.5         0.        ]\n",
      "  [-0.66666667  0.33333333]\n",
      "  [-3.         -2.        ]]\n",
      "\n",
      " [[-3.         -0.        ]\n",
      "  [ 2.          2.        ]\n",
      "  [ 0.         -1.        ]]]\n"
     ]
    }
   ],
   "source": [
    "#defines two 2x3x2 tensors, S and T\n",
    "S = np.array([[[1,0],[-2,1],[3,2]],[[-3,0],[2,2],[0,-1]]])\n",
    "T = np.array([[[-2,4],[3,3],[-1,-1]],[[1,-3],[1,1],[2,1]]])\n",
    "print('This is a 2x3x2 tensor S:')\n",
    "print(S)\n",
    "print()\n",
    "print('This is a 2x3x2 tensor T:')\n",
    "print(T)\n",
    "print()\n",
    "\n",
    "#element-wise operations between 2 tensors work the same way as the scalar operations\n",
    "print('This is the sum of S and T:')\n",
    "print(S+T)\n",
    "print()\n",
    "\n",
    "print('This is the difference of S and T:')\n",
    "print(S-T)\n",
    "print()\n",
    "\n",
    "print('This is the element-wise product of S and T:')\n",
    "print(S*T)\n",
    "print()\n",
    "\n",
    "print('This is the element-wise quotient of S and T:')\n",
    "print(S/T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate Functions\n",
    "So far we've talked a lot about tensors and their shapes, but one term we should define is an **axis**. A *tensor of order $n$* has $n$ axes. You can think of a single axis as \"direction\" along which you can traverse the tensor. Since a vector is a *tensor of order 1*, it only has one axis. You can only traverse it vertically (or horizontally. Depends on notion). A matrix has two axes. You can traverse it horizontally or vertically. In practice, different axes store different *types* of information. \n",
    "\n",
    "For example, if we have a single 360x720 image with RGB values for each pixel, then it would be stored as a *tensor of order 3* with a shape of (3,360,720) representing (#color channels, height, width). Abstractly, it doesn't matter what order the axes are in, i.e. (720,3,360) is completely valid, but in practice there are often standards for what axes represent what information. The most important thing is making sure you stay consistent with whatever standards you adopt. Here we're using PyTorch's \"channels first\" standard of putting the number of (color) channels first. Now, if we wanted to, we could also represent the image in this shape: (1,3,1,360,1,1,1,720). Would that be useful? Probably not. With that being said, sometimes it is quite helpful to include an extra axis in machine learning applications, but that'll be covered later.\n",
    "\n",
    "One of the best features of numpy is their incredibly well optimized system of aggregate functions. These functions also make tensors much more useful. An aggregate function is one that *aggregates*, or *reduces*, a certain value along certain axes. For example, perhaps the most common aggregate function is the `np.sum` function. By default, most aggregate functions will reduce along *all* axes, and return a single scalar, but you can specify along which axes they reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a tensor T:\n",
      "[[[1.59758585 1.84357114]\n",
      "  [5.31202091 7.47264605]]\n",
      "\n",
      " [[5.287028   4.9141707 ]\n",
      "  [0.06699536 3.06867199]]\n",
      "\n",
      " [[5.35072292 9.10017147]\n",
      "  [9.20637863 9.67819355]]]\n",
      "\n",
      "This is the sum of the elements of T:\n",
      "62.89815657790939\n",
      "\n",
      "This is the sum of the elements of T along the first axis:\n",
      "[[12.23533678 15.8579133 ]\n",
      " [14.5853949  20.2195116 ]]\n",
      "\n",
      "This is the sum of the elements of T along the first axis given by a for-loop:\n",
      "[[12.23533678 15.8579133 ]\n",
      " [14.5853949  20.2195116 ]]\n",
      "\n",
      "This is the sum of the elements of T along the second and third axes:\n",
      "[16.22582395 13.33686605 33.33546657]\n",
      "\n",
      "This is the sum of the elements of T along the second and third axes given by a for-loop:\n",
      "[16.22582395 13.33686605 33.33546657]\n",
      "\n",
      "This is the product of the elements of T:\n",
      "2709321.3614799776\n",
      "\n",
      "This is the product of the elements of T along the third axis:\n",
      "[[ 2.94526317 39.69485205]\n",
      " [25.98135808  0.20558679]\n",
      " [48.69249607 89.1011143 ]]\n",
      "\n",
      "This is maximum reduction of T along the second axis:\n",
      "[[5.31202091 7.47264605]\n",
      " [5.287028   4.9141707 ]\n",
      " [9.20637863 9.67819355]]\n",
      "This is the minimum reduction of T along the second axis:\n",
      "[[1.59758585 1.84357114]\n",
      " [0.06699536 3.06867199]\n",
      " [5.35072292 9.10017147]]\n",
      "\n",
      "This is the average reduction of T along the first axis:\n",
      "[[4.07844559 5.2859711 ]\n",
      " [4.8617983  6.7398372 ]]\n",
      "This is the median reduction of T along the first axis:\n",
      "[[5.287028   4.9141707 ]\n",
      " [5.31202091 7.47264605]]\n"
     ]
    }
   ],
   "source": [
    "#initializes a 3x2x2 tensor T with random values from 0 to 10\n",
    "T = np.random.random([3,2,2]) * 10\n",
    "print('This is a tensor T:')\n",
    "print(T)\n",
    "print()\n",
    "\n",
    "#sums the elements of T\n",
    "print('This is the sum of the elements of T:')\n",
    "print(np.sum(T))\n",
    "print()\n",
    "\n",
    "#sums the elements of T along the first axis\n",
    "print('This is the sum of the elements of T along the first axis:')\n",
    "print(np.sum(T, axis=0))\n",
    "print()\n",
    "\n",
    "#sums the elements of T along the first axis with a for-loop\n",
    "U = np.zeros([T.shape[1], T.shape[2]])\n",
    "for i in range(T.shape[0]):\n",
    "    for j in range(T.shape[1]):\n",
    "        for k in range(T.shape[2]):\n",
    "            U[j,k] += T[i,j,k]\n",
    "print('This is the sum of the elements of T along the first axis given by a for-loop:')\n",
    "print(U)\n",
    "print()\n",
    "\n",
    "#sums the elements of T along the second and third axes simultaneously\n",
    "print('This is the sum of the elements of T along the second and third axes:')\n",
    "print(np.sum(T, axis=(1,2)))\n",
    "print()\n",
    "\n",
    "#sums the elements of T along the second and third axes\n",
    "V = np.zeros(T.shape[0])\n",
    "for i in range(T.shape[0]):\n",
    "    for j in range(T.shape[1]):\n",
    "        for k in range(T.shape[2]):\n",
    "            V[i] += T[i,j,k]\n",
    "print('This is the sum of the elements of T along the second and third axes given by a for-loop:')\n",
    "print(V)\n",
    "print()\n",
    "\n",
    "#multiplies the elements of T together\n",
    "print('This is the product of the elements of T:')\n",
    "print(np.prod(T))\n",
    "print()\n",
    "\n",
    "#multiplies the elements of T along the third axis\n",
    "#this produces a 3x2 matrix consisting of the columns of each matrix of T element-wise multiplied together\n",
    "print('This is the product of the elements of T along the third axis:')\n",
    "print(np.prod(T, axis=2))\n",
    "print()\n",
    "\n",
    "#finds the maximum and minimum elements of T along the 2nd axis\n",
    "#produces 3x2 matrices consisting of the maximal/minimal element of each column of the matrices composing T\n",
    "print('This is maximum reduction of T along the second axis:')\n",
    "print(np.max(T, axis=1))\n",
    "print('This is the minimum reduction of T along the second axis:')\n",
    "print(np.min(T, axis=1))\n",
    "print()\n",
    "\n",
    "#finds the average and median of the elements of T along the first axis\n",
    "print('This is the average reduction of T along the first axis:')\n",
    "print(np.mean(T, axis=0))\n",
    "print('This is the median reduction of T along the first axis:')\n",
    "print(np.median(T, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiling\n",
    "\n",
    "Tiling is a technique used when creating or modifying tensors which essentially repeats tensors along certain axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a vector v of length 6 with values from 0 to 5:\n",
      "[0. 1. 2. 3. 4. 5.]\n",
      "\n",
      "This is a vector of length 18 constructed by tiling v 3 times along its axis\n",
      "[0. 1. 2. 3. 4. 5. 0. 1. 2. 3. 4. 5. 0. 1. 2. 3. 4. 5.]\n",
      "\n",
      "This is a 6x6 matrix constructed by tiling v 6 times along a new first axis:\n",
      "[[0. 1. 2. 3. 4. 5.]\n",
      " [0. 1. 2. 3. 4. 5.]\n",
      " [0. 1. 2. 3. 4. 5.]\n",
      " [0. 1. 2. 3. 4. 5.]\n",
      " [0. 1. 2. 3. 4. 5.]]\n",
      "\n",
      "This is a 2x4x6 tensor constructed by tiling v 2 times along a new first axis and 4 times along a new second axis:\n",
      "[[[0. 1. 2. 3. 4. 5.]\n",
      "  [0. 1. 2. 3. 4. 5.]\n",
      "  [0. 1. 2. 3. 4. 5.]\n",
      "  [0. 1. 2. 3. 4. 5.]]\n",
      "\n",
      " [[0. 1. 2. 3. 4. 5.]\n",
      "  [0. 1. 2. 3. 4. 5.]\n",
      "  [0. 1. 2. 3. 4. 5.]\n",
      "  [0. 1. 2. 3. 4. 5.]]]\n",
      "\n",
      "This is a 6x6 matrix with values from 0 to 10:\n",
      "[[ 0.  1.  2.  3.  4.  5.]\n",
      " [ 1.  2.  3.  4.  5.  6.]\n",
      " [ 2.  3.  4.  5.  6.  7.]\n",
      " [ 3.  4.  5.  6.  7.  8.]\n",
      " [ 4.  5.  6.  7.  8.  9.]\n",
      " [ 5.  6.  7.  8.  9. 10.]]\n",
      "\n",
      "This is a 2x2x6x6 tensor constructed by tiling A 2 times along two new axes:\n",
      "[[[[ 0.  1.  2.  3.  4.  5.]\n",
      "   [ 1.  2.  3.  4.  5.  6.]\n",
      "   [ 2.  3.  4.  5.  6.  7.]\n",
      "   [ 3.  4.  5.  6.  7.  8.]\n",
      "   [ 4.  5.  6.  7.  8.  9.]\n",
      "   [ 5.  6.  7.  8.  9. 10.]]\n",
      "\n",
      "  [[ 0.  1.  2.  3.  4.  5.]\n",
      "   [ 1.  2.  3.  4.  5.  6.]\n",
      "   [ 2.  3.  4.  5.  6.  7.]\n",
      "   [ 3.  4.  5.  6.  7.  8.]\n",
      "   [ 4.  5.  6.  7.  8.  9.]\n",
      "   [ 5.  6.  7.  8.  9. 10.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  1.  2.  3.  4.  5.]\n",
      "   [ 1.  2.  3.  4.  5.  6.]\n",
      "   [ 2.  3.  4.  5.  6.  7.]\n",
      "   [ 3.  4.  5.  6.  7.  8.]\n",
      "   [ 4.  5.  6.  7.  8.  9.]\n",
      "   [ 5.  6.  7.  8.  9. 10.]]\n",
      "\n",
      "  [[ 0.  1.  2.  3.  4.  5.]\n",
      "   [ 1.  2.  3.  4.  5.  6.]\n",
      "   [ 2.  3.  4.  5.  6.  7.]\n",
      "   [ 3.  4.  5.  6.  7.  8.]\n",
      "   [ 4.  5.  6.  7.  8.  9.]\n",
      "   [ 5.  6.  7.  8.  9. 10.]]]]\n"
     ]
    }
   ],
   "source": [
    "#generates a vector of length 6 with values from 0 to 5\n",
    "v = np.linspace(0,5,6)\n",
    "print('This is a vector v of length 6 with values from 0 to 5:')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#constructs a vector length 18 by tiling v\n",
    "print('This is a vector of length 18 constructed by tiling v 3 times along its axis')\n",
    "print(np.tile(v, 3))\n",
    "print()\n",
    "\n",
    "#constructs a 6x6 matrix by tiling v\n",
    "print('This is a 6x6 matrix constructed by tiling v 6 times along a new first axis:')\n",
    "print(np.tile(v, [5,1]))\n",
    "print()\n",
    "\n",
    "#constructs a 2x4x6 tensor by tiling v\n",
    "print('This is a 2x4x6 tensor constructed by tiling v 2 times along a new first axis and 4 times along a new second axis:')\n",
    "print(np.tile(v, [2,4,1]))\n",
    "print()\n",
    "\n",
    "#initializes a 6x6 matrix with values from 0 to 10\n",
    "A = np.stack([np.linspace(i,i+5,6) for i in range(0,6,1)], axis=1)\n",
    "print('This is a 6x6 matrix with values from 0 to 10:')\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "#constructs a 2x2x6x6 tensor with values formed by tiling A\n",
    "print('This is a 2x2x6x6 tensor constructed by tiling A 2 times along two new axes:')\n",
    "print(np.tile(A, [2,2,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms and Metrics\n",
    "\n",
    "The mathematical definition of **distance** is formalized as a **metric**. A metric is a function $d:X\\times X\\rightarrow[0,\\infty)$ that obeys 4 properties: \n",
    "$$\n",
    "\\forall x,y,z\\in X, d(x,y)\\geq 0\\\\\n",
    "d(x,y)=0 \\iff x=y\\\\\n",
    "d(x,y)=d(y,x)\\\\\n",
    "d(x,z)\\leq d(x,y)+d(y,z)\\\\\n",
    "$$\n",
    "\n",
    "You already have some experience with metrics, whether you know it by that name or not. How close are $3,7$? Well they're only $4$ units away. What about $-3,-7$? $4$ as well. Same as $7,3$. When dealing with real numbers, the metric we usually use is  $d(x,y)=|x-y|$, which perfectly obeys all our established properties. There's nothing stopping us from having a metric on things other than real numbers, for example, vectors! The most classic example is probably *euclidean distance*, defined as the square root of the sum of squares. To define our *euclidean distance* metric $d: \\mathbb{R}^n\\times\\mathbb{R}^n\\rightarrow [0,\\infty)$, we say that $\\forall x,y\\in\\mathbb{R}^n, d(x,y)=\\sqrt{\\sum_{i=1}^n(x_i-y_i)^2}$\n",
    "\n",
    "\n",
    "Every metric also gives us what is known as a **norm**. You can think of it as a generalization of the absolute value function. In fact, $f(x)=|x|$ is a proper norm. Given a metric $d$, its associated norm $f(x)=d(0,x)$, or its distance from zero (or whatever represents \"zero\", e.g. the zero vector). Similarly, you can generally induce a distance from a norm by defining $d(x,y)=f(x-y)$, so we'll usually discuss these two semi-interchangeably. When you've already established a norm that you're working in, or it is otherwise obvious, then instead of referring to the actual norm function, you can instead refer to the double-bar notation: $f(x)=\\|x\\|$. \n",
    "\n",
    "Norms and metrics are incredibly important in data science, and machine learning especially, since they define the idea of **distance**. That notion of **distance** also defines how similar/dissimilar data is to other data. This results in determining the shape, or *topology*, of the space that your data exists in. It's all very abstract right now, but we'll see concrete examples of this in later workshops. \n",
    "\n",
    "A special family of norms exists, known as $L^p$ norms $L^p:\\mathbb{R}^n\\rightarrow[0,\\infty)$, defined by: $$L^p(x)=(\\sum_{i=1}^n(|x_i|)^p)^{1/p}$$This is a widely used family, including two very heavily used norms: $L^1(x)=\\sum_{i=1}^n|x_i|$, and $L^2(x)=(\\sum_{i=1}^n|x_i|^2)^{1/2}=\\sqrt{\\sum_{i=1}^nx_i^2}$\n",
    "\n",
    "$L^2$ is the familiar euclidean norm, while $L^1$ is a norm popularly used in image analysis. Another nice property is that in $\\mathbb{R}^n, L^2(x)=$, \n",
    "\n",
    "Let's check out what these things look like when coded up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a vector u:\n",
      "[1 0 1]\n",
      "\n",
      "This is a vector v:\n",
      "[-1  0  1]\n",
      "\n",
      "This is the euclidean distance between u and v:\n",
      "The difference between a and b is  [2 0 0]\n",
      "Element-wise squaring gets us   [4 0 0]\n",
      "The sum of those squares is  4\n",
      "The square root of the sum is  2.0\n",
      "Therefore the euclidean distance is: \n",
      "2.0\n",
      "\n",
      "This is a vector u:\n",
      "[1 2 3 4]\n",
      "\n",
      "This is a vector v:\n",
      "[-1  4 -3  5]\n",
      "\n",
      "This is the euclidean distance between u and v:\n",
      "The difference between a and b is  [ 2 -2  6 -1]\n",
      "Element-wise squaring gets us   [ 4  4 36  1]\n",
      "The sum of those squares is  45\n",
      "The square root of the sum is  6.708203932499369\n",
      "Therefore the euclidean distance is: \n",
      "6.708203932499369\n",
      "\n",
      "This is a vector u:\n",
      "[1 2 3 4]\n",
      "\n",
      "This is a vector v:\n",
      "[-1  4 -3  5]\n",
      "\n",
      "This is the L1 norm of v:\n",
      "13.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initializes 2 vectors, u and v, of length 3\n",
    "u = np.array([1,0,1])\n",
    "v = np.array([-1,0,1])\n",
    "print('This is a vector u:')\n",
    "print(u)\n",
    "print()\n",
    "print('This is a vector v:')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "def euclideanDistance(a,b, debug=False):\n",
    "    difference = a-b\n",
    "    squaredDifference = difference * difference\n",
    "    sum = np.sum(squaredDifference)\n",
    "    distance = np.sqrt(sum)\n",
    "    if(debug):\n",
    "        print('The difference between a and b is ', difference)\n",
    "        print('Element-wise squaring gets us  ', squaredDifference)\n",
    "        print('The sum of those squares is ', sum)\n",
    "        print('The square root of the sum is ', distance)\n",
    "        print('Therefore the euclidean distance is: ')\n",
    "    return distance\n",
    "\n",
    "#finds the euclidean distance between u and v\n",
    "print('This is the euclidean distance between u and v:')\n",
    "print(euclideanDistance(u, v, debug=True))\n",
    "print()\n",
    "\n",
    "#initializes two vectors, u and v, of length 4\n",
    "u = np.array([1,2,3,4])\n",
    "v = np.array([-1,4,-3,5])\n",
    "print('This is a vector u:')\n",
    "print(u)\n",
    "print()\n",
    "print('This is a vector v:')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#finds the euclidean distance between u and v\n",
    "print('This is the euclidean distance between u and v:')\n",
    "print(euclideanDistance(u, v, debug=True))\n",
    "print()\n",
    "\n",
    "print('This is a vector u:')\n",
    "print(u)\n",
    "print()\n",
    "print('This is a vector v:')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#finds the L1 norm of v\n",
    "print('This is the L1 norm of v:')\n",
    "print(np.linalg.norm(v,1))\n",
    "print()\n",
    "\n",
    "#finds the L1 norm of v by following the formula\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
