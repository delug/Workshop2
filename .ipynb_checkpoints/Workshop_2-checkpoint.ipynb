{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 2 - The Mathematics Behind Data Science\n",
    "\n",
    "This workshop will cover material ranging from what a vector is all the way to $L^p$ norms, loss functions, and dimensionality reduction! We want to emphasize that a strong math background is not required for this workshop, as we'll be presenting the material in a beginner oriented, hands-on way. That means that we will introduce material both in terms of what you may code up in any given project, and the abstract math objects which represent them. In the simplest case, a vector can be described as a 1D array, but that's not enough to justify many of the techniques employed in deep learning. In order to extend that, we will dive into the math that powers the code. Specifically, this workshop aims to cover:\n",
    "\n",
    "- Tensors\n",
    "    - Vectors\n",
    "    - Matrices\n",
    "    - Tensors (multi-dimensional arrays)\n",
    "\n",
    "- Functional Operations\n",
    "    - Sum/Product reductions\n",
    "    - Tensor tiling\n",
    "- Norms\n",
    "    - Euclidean distance\n",
    "    - Metrics\n",
    "    - $L^1$, $L^2$, $L^p$ norms\n",
    "- Loss Functions\n",
    "    - Mean Squared Error\n",
    "    - Mean Absolute Error\n",
    "- Dimensionality Reduction\n",
    "    - Principle Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - NumPy Arrays, Vectors, Matrices, and Tensors\n",
    "\n",
    "### Arrays\n",
    "\n",
    "Suppose you wanted to store an image in a Python variable. A digital image is really a list of numbers, the brightnesses of each pixel on screen. We could store each number in one long list, but this is hard to work with and a picture has a 2-dimensional structure that would be missing. In Python, we can \"nest\" lists, and represent our picture as a list of rows which are themselves a list of pixels. We can apply the `[]` index operator once to access a particular row, and apply it a second time to access a particular pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list = [[0,1,2],\n",
    "               [4,5,6],\n",
    "               [7,8,9]]\n",
    "\n",
    "print(nested_list)\n",
    "print(nested_list[0])\n",
    "print(nested_list[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NumPy package for Python has an array class, `ndarray` ($n$-dimensional array), that we will use instead of lists. NumPy also includes many array operations that will be useful, and these are much faster than analogs that could be written for nested Python lists. We can create these arrays by importing `numpy` (usually shortened to `np` with the `as` keyword) and using the `array` function. This function lets us construct an NumPy array with a Python list with any level of nesting. An important restriction is that the array must be \"rectangular,\" in that each nested list must contain the same number of elements (i.e. each row must have the same number of pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True) # Convenience setting. Can safely ignore\n",
    "\n",
    "print('Array from a list with no nesting:')\n",
    "\n",
    "a = np.array([1,2,3,4]) # create a numpy array using a python list\n",
    "\n",
    "print(a) # Note the numpy array prints without commas \",\"\n",
    "print(a[1]) # Access elements just as you regularly would\n",
    "\n",
    "print('\\nArray from the nested list:')\n",
    "x = np.array(nested_list)\n",
    "\n",
    "print(x)\n",
    "print(x[0])\n",
    "print(x[2,1]) # NOTE: index a numpy array by using one pair of brackets and indices separated by commas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors\n",
    "\n",
    "Vectors form the basis for the vast majority of data-science but are especially significant in deep learning! To begin with, vectors are the most fundamental way by which we represent data. Ultimately, a vector can be considered a \"list\" of numbers, with each number being assigned an **index**. These indices give us a way to explicitly refer to the individual elements composing the vector.\n",
    "\n",
    "Vectors of the same length can be added together by adding the corresponding entries of each vectors. Vectors can also be multiplied by scalars (real numbers) which involves multiplying each component of a vector by the scalar. Working with vectors as NumPy arrays lets us perform these operations with the usual `+` and `*` operators! (Try doing this with Python lists, what happens?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([3,1,2])\n",
    "b = np.array([-1,4,0])\n",
    "\n",
    "print(a + b) # vector addition\n",
    "print(3 * b) # scalar multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, a vector is referred to as $N$ *dimensional* if and only if it has exactly $N$ *elements*. A vector is also a *tensor of order 1*, or alternatively, a *first order tensor* (we'll use the first name scheme). We're going to draw a sharp distinction between dimensionality and order. In a lot of data science literature, these two are generally interchangeably referred to as *dimension*, but that leads to way too many headaches for us to justify using it. We've defined dimensionality for our purposes, so what is order? Well, roughly speaking the order of a vector/matrix/tensor is **the number of indices required to specify which element you're talking about**. So again, a vector is a tensor of order 1, meaning we need only one index to exactly specify which element we want to access. This is shown in our last example! \n",
    "\n",
    "For notation, we'll write out a vector, such as \"a\" in the code above like this: $\\vec a = \\left<1,2,3,4 \\right>$. Other acceptable notations include: $a=[1,2,3,4]^\\intercal$, or $a=\\begin{bmatrix}1\\\\2\\\\3\\\\4\\end{bmatrix}$. We use the first one because it's honestly just easier, and doesn't mess up vertical spacing unlike *some* methods.\n",
    "\n",
    "Now, suppose we have $\\vec{v}$, an $N$ dimensional vector, and suppose every element will be a real number, then we say that $\\vec{v}\\in\\mathbb{R}^N$ read as *v is an element of R-N* (not *R to the power of N*). If your elements are instead integers, the statement would be $\\vec{v}\\in\\mathbb{Z}^N$. In general, if your elements belong to a set $S$, then you write $\\vec{v}\\in S^N$\n",
    "\n",
    "So if we had a vector $\\vec{u}$ which had 6 elements which are real numbers, we'd write $\\vec{u}\\in\\mathbb{R}^6$. If it had 6 integers instead, $\\vec{u}\\in\\mathbb{Z}^6$. Now suppose 4 of those elements were real numbers, and 2 were integers, then $\\vec{u}\\in\\mathbb{R}^4\\times\\mathbb{Z}^2$. Note that $\\mathbb{R}^6=\\mathbb{R}^3\\times\\mathbb{R}^3$. This language gives us much more flexibility in discussing more complex ideas.\n",
    "\n",
    "**Note:** In mathematical notation, it is common to write a vector as a letter with an arrow above it, like $\\vec{v}$. For convenience, we will stop writing the arrow above the vector, since in context it is generally clear whether or not a variable refers to a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices\n",
    "\n",
    "Many of you may be familiar with matrices, but most of you will have only seen them in passing. A matrix can be defined in many ways, but the most programmer-friendly description is as a **collection of elements**, much like how we defined our vectors! With matrices, however, we need two indices in order to specify which element we're talking about.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x) # our picture\n",
    "\n",
    "print(x[0])   # first row\n",
    "print(x[0,1]) # first row, second column\n",
    "print(x[2,2]) # third row, third column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above example, we require two indices (e.g 0,1) in order to specify a particular element, hence this is a *tensor of order 2*. So what exactly happened when we only specified one index? Well, it returned the first row of the matrix. This leads to another interpretation of a matrix: **a list of vectors**. If you have vectors $v_1,v_2,v_3$, then you can construct a matrix $A=[v_1,v_2,v_3]^\\intercal$ where our three vectors serve as *rows* in the matrix. In the above code, $v_1=\\left<1,2,3\\right>$, $v_2=\\left<4,5,6\\right>$, $v_3=\\left<7,8,9\\right>$. This is why when we run `print(x[0])` we get $v_1$. We can also loop over a matrix and get its rows in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in x:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In math literature we generally construct matrices using vectors as their *columns*, and this can cause a bit of confusion. In practice it doesn't matter all too much though, since you can always use a handy function to change your rows to columns and vice versa known as the *transpose*, usually denoted as $A^\\intercal$. In fact, when we defined $A$ originally, we used the transpose so that we could set each $v_i$ as a row instead of column. Let's see it in action below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x) # Regular\n",
    "print()\n",
    "print(x[0]) # First row of x\n",
    "\n",
    "print()\n",
    "y=x.transpose()\n",
    "print(y) # Switch rows with columns\n",
    "print()\n",
    "print(y[0]) # First row of y, but first column of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undeniably the most common operation regarding matrices is *matrix multiplication*. This can be a little unintuitive at first, but we hope to help explain the weird behaviour. First off, matrices need to have the right kind of shape before being multiplied together. We say a matrix $A$ is $m\\times n$ when it has $m$ rows and $n$ columns. If we want to multiply matrices $A$ and $B$ to get $C=AB$, then $B$ needs to have the same number of rows as $A$ has columns. So if $A$ is $m\\times n$ then $B$ must be $n\\times l$. Then $C$ would end up being an $m\\times l$ matrix. An easy, albeit mathematically lax, way to figure out shape compatibilities for matrices is that the shape of $C=AB$ is $(m\\times n)\\times(n\\times l)\\implies m\\times (n\\times n) \\times l \\implies m\\times l$. You can imagine it as the inner variable $n$ *canceling out*. Which also helps to remind you that $B$ needs to have the same number of rows as $A$ has columns, else the inner dimensions wouldn't be the same and hence wouldn't cancel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2],[3,4],[1,4]]) # A is 3x2\n",
    "B = np.array([[5,6],[7,8]]) # B is 2x2\n",
    "\n",
    "print('This is the first matrix:')\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "print('This is the second matrix:')\n",
    "print(B)\n",
    "print()\n",
    "\n",
    "print('This is their product:')\n",
    "\n",
    "#multiplies the 2 matrices together\n",
    "print(np.dot(A,B)) # AB is (3x2)x(2x2)=>3x(2x2)x2=>3x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what exactly is going on here? Let's start with a simple example using a $3x2$ matrix $A=[\\vec a_1,\\vec a_2]$ where $\\vec a_i\\in\\mathbb{R}^3$ (note since we're not using the transpose, each $\\vec a_i$ is a *column* of $A$), and $\\vec b= \\left<b_1,b_2 \\right>$, a $2x1$ matrix, which is alternatively just a *vector*, then $C=A\\vec b$ is going to be $3\\times 1$, so it'll also be a vector! We can define $A\\vec b=\\sum_{i=1}^2b_i\\vec a_i=b_1\\vec a_1 + b_2\\vec a_2$.\n",
    "\n",
    "**Note**: We switched back to arrow notation here since we'll be juggling between vectors and scalars, and so it's not entirely obvious in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes a 3x2 matrix A\n",
    "A = np.array([[1,1],[3,0],[2,1]])\n",
    "\n",
    "#initializes a vector b of length 2\n",
    "b = np.array([-1,2])\n",
    "\n",
    "print('This is a 3x2 matrix A:')\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "print('This a length 2 vector b:')\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "#multiplies Ab\n",
    "print('This is their product Ab:')\n",
    "print(np.dot(A,b))\n",
    "print()\n",
    "\n",
    "#demonstrates the internal steps in calculating Ab\n",
    "b1a1 = b[0] * np.transpose(A)[0]\n",
    "print('This is the first value of b times the first column of A:')\n",
    "print(b1a1)\n",
    "print()\n",
    "\n",
    "b2a2 = b[1] * np.transpose(A)[1]\n",
    "print('This is the second value of b times the second column of A:')\n",
    "print(b2a2)\n",
    "print()\n",
    "\n",
    "print('This is their sum:')\n",
    "print(b1a1 + b2a2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes a 5x4 matrix A\n",
    "A = np.array([[1,1,1,1],[1,0,1,0],[1,2,3,4],[0,0,0,0],[-1,1,-1,0]])\n",
    "print('This is a 5x4 matrix A:')\n",
    "print(A)\n",
    "print()\n",
    "print(\"Shape of A: \",A.shape) # Prints the shape of A\n",
    "\n",
    "\n",
    "#initializes a vector b of length 4\n",
    "b = np.array([1,0,-1,2])\n",
    "print('This is a length 4 vector b:')\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "#multiplies Ab\n",
    "print('This is the product Ab:')\n",
    "print(np.dot(A,b))\n",
    "print()\n",
    "\n",
    "#initializes a 6x6 identity matrix\n",
    "I = np.eye(6)\n",
    "print('This is a 6x6 identity matrix I:')\n",
    "print(I)\n",
    "print()\n",
    "\n",
    "#initializes a vector v of length 6\n",
    "v = np.linspace(0,5,6)\n",
    "print('This is a length 6 vector v:')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#multiplies Iv. Note the result of multiplication by the \"identity matrix\"\n",
    "print('This is the product Iv:')\n",
    "print(np.dot(I,v))\n",
    "print()\n",
    "\n",
    "#The next print statement would fail because it multiplies a 5x4 matrix A against a length 6 vector v\n",
    "#print(np.dot(A,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've only multiplied a matrix against a vector, but what about against another matrix? Well, once again, thinking about a matrix as a collection of vectors will help us out. Instead of getting the whole matrix in one shot, we'll do it column by column. Take the same $A$ as defined before, but now consider a matrix $B=[\\vec b_1, \\vec b_2, \\vec b_3]$ with $\\vec b_i\\in\\mathbb{R}^2$, so $B$ is a $2\\times3$, and since $A$ is a $3\\times2$, our resulting matrix $C=AB$ will be a $3\\times3$. We'll first calculate $\\vec c_1=\\sum_{i=1}^3b_{1,i}\\vec a_i=b_{1,1}\\vec a_1 + b_{1,2}\\vec a_2 +b_{1,3}\\vec a_3 $ where $b_{j,i}$ refers to the $i^\\text{th}$ element of vector $b_j$. Hopefully this looks at least a *little* familiar. This is the exact same process as we had for our matrix times vector case! So $\\vec c_i=A\\vec b_i$. There are many other ways to imagine matrix multiplication, each with their own merits, but this definition is what we recommend for most people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes a 2x3 matrix A\n",
    "A = np.array([[1,0,1],[-1,1,0]])\n",
    "print('This is a 2x3 matrix A:')\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "#initializes a 3x2 matrix B\n",
    "B = np.array([[3,2],[4,1],[0,3]])\n",
    "print('This is a 3x2 matrix B:')\n",
    "print(B)\n",
    "print()\n",
    "\n",
    "#multiplies AB\n",
    "print('This is the product AB:')\n",
    "print(np.dot(A,B))\n",
    "print()\n",
    "\n",
    "#multiplies A*b1\n",
    "Ab1 = np.dot(A,B[:,0])\n",
    "print('This is the product A*b1:')\n",
    "print(Ab1)\n",
    "print()\n",
    "\n",
    "#multiplies A*b2\n",
    "Ab2 = np.dot(A,B[:,1])\n",
    "print('This is the product A*b2:')\n",
    "print(Ab2)\n",
    "print()\n",
    "\n",
    "#constructs [A*b1, A*b2]\n",
    "Ab12 = np.stack([Ab1,Ab2], 1)\n",
    "print('This is the matrix with columns A*b1 and A*b2:')\n",
    "print(Ab12)\n",
    "print()\n",
    "\n",
    "#multiplies BA\n",
    "print('This is the product BA:')\n",
    "print(np.dot(B,A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** a very special case of matrix multiplication is when given two vectors $a,b$, we calculate $c=b^\\intercal a=\\sum_{i=1}^na_ib_i$, which is perhaps better known as the dot product! So $c=b^\\intercal a=a\\cdot b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are the greatest abstraction of vectors and matrices we'll be covering during this workshop. Tensors are best discussed in terms of their *order*. As a reminder, we defined *order* as the number of indices required to specify a single element in the object. We've already covered tensors of order 1 and 2 so let's look at *tensors of order 3*. Just as matrices were considered **lists of vectors**, a *tensors of order 3* can be considered a **list of matrices**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes a 4x3x2 tensor with the values from 1 to 24\n",
    "A1=np.array([[1,2],[3,4],[5,6]])\n",
    "A2=np.array([[7,8],[9,10],[11,12]])\n",
    "A3=np.array([[13,14],[15,16],[17,18]])\n",
    "A4=np.array([[19,20],[21,22],[23,24]])\n",
    "\n",
    "print(\"The matrices:\")\n",
    "print(A1)\n",
    "print(A2)\n",
    "print(A3)\n",
    "print(A4)\n",
    "\n",
    "t = np.array([A1,A2,A3,A4])\n",
    "print('\\nThis is t:')\n",
    "print(t)\n",
    "\n",
    "#prints the shape of the tensor t\n",
    "#note all\n",
    "print(t.shape)\n",
    "print()\n",
    "\n",
    "#note that to access a specific element, an index must be specified for each axis\n",
    "print('These are some elements of t:')\n",
    "print(t[0,0,0]) #element located in the first position of each axis\n",
    "print(t[1,0,0]) #element located in the second position of first axis, first position of second and third axes\n",
    "print(t[0,1,0]) #element located in the first position of first axis, second position of second axis, first position of third axis\n",
    "print(t[0,0,1]) #element located in the first position of first and second axes, second position of third axis\n",
    "print(t[3,2,1]) #element located in the fourth position of first axis, third position of second axis, and second position of third axis\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no reason to stop there though. In general we have *tensors of order $n$* which can be seen as lists of *tensors of order $(n-1)$* (...or as matrices of *tensors of order $(n-2)$*, or as a *tensors of order 3* of *tensors of order $(n-3)$*). Let's look at a *tensor of order 5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes a 2x2x2x2x2 tensor with values from 1 to 32\n",
    "b = np.array([\n",
    "    [\n",
    "        [\n",
    "            [[1,2],[3,4]],\n",
    "            [[5,6],[7,8]]\n",
    "        ],\n",
    "        [\n",
    "            [[9,10],[11,12]],\n",
    "            [[13,14],[15,16]]]\n",
    "    ],\n",
    "    [\n",
    "        [\n",
    "            [[17,18],[19,20]],\n",
    "            [[21,22],[23,24]]\n",
    "        ],\n",
    "        [\n",
    "            [[25,26],[27,28]],\n",
    "            [[29,30],[31,32]]\n",
    "        ]\n",
    "    ]\n",
    "])\n",
    "print('This is a big tensor\\'s shape and values')\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a pain to write out every value of a tensor, surrounded by the appropriate brackets, so for certain cases there are very handy initialization tools for creating some commonly used tensors. As well as commonly used tensors, numpy allows us to specify what types of elements they will contain. We've been assuming all our elements are real numbers, or equivelantly, `float` values. In general they can be 32-bit integers (`int32`), 64-bit integers (`int64`), 32-bit floats (`float32`), 64-bit floats (`float64`), 128-bit complex numbers (`complex`), booleans (`bool`), objects (`object`), strings (`string_`), or unicode characters (`unicoded_`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes a 2x2x5 tensor with the boolean value 0 in each position\n",
    "z = np.zeros([2,2,5], dtype=bool)\n",
    "print('This is a tensor of boolean zeroes:')\n",
    "print(z)\n",
    "print()\n",
    "\n",
    "#initializes a 2x2x5 tensor with the 32-bit float value 1 in each position\n",
    "o = np.ones([2,2,5], dtype=np.float32)\n",
    "print('This is a tensor of 32-bit float ones:')\n",
    "print(o)\n",
    "print()\n",
    "\n",
    "#initializes a 2x2x5 tensor with the 64-bit integer value of c in each position\n",
    "c = 42\n",
    "f = np.full([2,2,5], c, np.int64)\n",
    "print('This is a tensor with a constant 64-bit integer value in each position:')\n",
    "print(f)\n",
    "print()\n",
    "\n",
    "#initializes a 2x2x5 tensor with random values in each position\n",
    "r = np.random.random([2,2,5])\n",
    "print('This is a tensor with random values in each position:')\n",
    "print(r)\n",
    "print()\n",
    "\n",
    "#constructs a tensor from other tensors\n",
    "b = np.stack([z,o,f,r])\n",
    "print('This is a tensor formed by stacking the others along a new axis:')\n",
    "print(b)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we'll take a look at the different ways that you can manipulate tensors, starting with simple arithmetic operations. They can be broadly divided into two categories: scalar, and tensor. Scalar arithmetic is just arithmetic that involves one tensor and a scalar. Tensor arithmetic happens between two tensors. Here are some basic scalar arithmetic operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes a 2x2x2 tensor with specified values\n",
    "v = np.array([[[2, 3.5], [4, 2]], [[1.5, 1], [2, 1]]])\n",
    "print('This is v:')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#constructs a new 2x2x2 tensor formed by adding 2 to each of the entries in v\n",
    "vAdd = v + 2\n",
    "print('This is v+2:')\n",
    "print(vAdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigns the name w to v, creating a shallow copy\n",
    "w = v\n",
    "\n",
    "#constructs a new tensor x with the values of v, creating a deep copy\n",
    "x = np.copy(v)\n",
    "\n",
    "print('This is v: ')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#adds 1 to each value in v\n",
    "v += 1\n",
    "print('This is v after adding 1')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#divides each value in v by 2\n",
    "v /= 2\n",
    "print('This is v after dividing by 2')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "print('This is w, changed by the operations performed on v: ')\n",
    "print(w)\n",
    "print()\n",
    "\n",
    "print('This is x, unchanged by the operations performed on v: ')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These occur as element-wise operations. We say an operation is element wise when instead of applying the operation to the tensors, we can directly apply it to their elements instead. For example, if we have a tensor $A$ and a scalar $k$ so that $C=A+k$ then we say that $c_{i...j}=a_{i...j}+k$, and so the operation is instead defined for every individual element. Tensor arithmetic can also be element wise. Suppose we have tensors $A,B$ and define $C=A*B$, then we can instead define $C$ element-wise like this: for every element $c_{i...j}=a_{i...j}*b_{i...j}$. \n",
    "\n",
    "**Note:** here we use $*$ to represent element-wise multiplication instead of proper tensor/matrix multiplication which is instead denoted by placing the matrices/tensors adjacent to eachother, e.g. $C=AB$ \n",
    "\n",
    "Below are some examples of tensor arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines two 2x3x2 tensors, S and T\n",
    "S = np.array([[[1,0],[-2,1],[3,2]],[[-3,0],[2,2],[0,-1]]])\n",
    "T = np.array([[[-2,4],[3,3],[-1,-1]],[[1,-3],[1,1],[2,1]]])\n",
    "print('This is a 2x3x2 tensor S:')\n",
    "print(S)\n",
    "print()\n",
    "print('This is a 2x3x2 tensor T:')\n",
    "print(T)\n",
    "print()\n",
    "\n",
    "#element-wise operations between 2 tensors work the same way as the scalar operations\n",
    "print('This is the sum of S and T:')\n",
    "print(S+T)\n",
    "print()\n",
    "\n",
    "print('This is the difference of S and T:')\n",
    "print(S-T)\n",
    "print()\n",
    "\n",
    "print('This is the element-wise product of S and T:')\n",
    "print(S*T)\n",
    "print()\n",
    "\n",
    "print('This is the element-wise quotient of S and T:')\n",
    "print(S/T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Creating, Saving, and Loading NumPy Arrays\n",
    "\n",
    "There are many common and convenient ways to create NumPy arrays that do not refer to Python lists. The following are built-in NumPy functions. Check the [NumPy docs](https://numpy.org/devdocs/reference/routines.array-creation.html) for more information.\n",
    "\n",
    "- `zeros` all zeros\n",
    "- `ones` all ones\n",
    "- `linspace` linearly spaced numbers\n",
    "- `random` random numbers\n",
    "- `eye` an identity matrix\n",
    "- `full` all one particular value\n",
    "- `_like` methods\n",
    "\n",
    "Numpy arrays can conveniently be saved as .npy files and loaded again for later use. This is especially helpful since very often data is represented using numpy arrays, it allows you to save them directly as .npy files instead of, for example, reading a .png, extracting pixel values, constructing a python array, and finally constructing an appropriate numpy array every single time you need to work with your data. Instead, do it once, save it, and then only ever load the numpy arrays directly. It is an immense workflow optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads an array called images.npy from present working directory\n",
    "loaded = np.load('images.npy')\n",
    "print('This is the first image from an array called images.npy:')\n",
    "print(loaded[0])\n",
    "print()\n",
    "\n",
    "#performs a transformation on the array\n",
    "newImages = loaded * 2\n",
    "print('This is the transformed first image:')\n",
    "print(newImages[0])\n",
    "print()\n",
    "\n",
    "#saves the array under a different name\n",
    "np.save('editedImages.npy', newImages)\n",
    "print('the array has been saved as editedImages.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Slicing\n",
    "\n",
    "NumPy array slicing is a direct extension of python slicing, and will be a very common manipulation for most data scientists. Recall that Python list can be \"sliced\" using the `[]` operator, in the format that `a[start:stop]` gives elements of the list `a` starting at the `start` index and ending before the `stop` index. For NumPy arrays, we can use the slicing operator in each axis, separated by commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes a 2x3x5 tensor T\n",
    "T = np.stack([np.stack([np.linspace(15*j+5*i+1,15*j+5*i+5,5) for i in range(3)]) for j in range(2)])\n",
    "print('This is a 2x3x5 tensor T:')\n",
    "print(T)\n",
    "print()\n",
    "\n",
    "#note that the trivial slice of T is simply T itself\n",
    "print('This is the :,:,: slice of T:')\n",
    "print(T[:,:,:])\n",
    "print()\n",
    "\n",
    "#slices T to isolate different matrices\n",
    "print('This is the 0,:,: slice of T:')\n",
    "print(T[0,:,:])\n",
    "print()\n",
    "\n",
    "print('This is the :,0,: slice of T:')\n",
    "print(T[:,0,:])\n",
    "print()\n",
    "\n",
    "#slices T to isolate tensors without reducing the order\n",
    "print('This is the :,0:2,: slice of T:')\n",
    "print(T[:,0:2,:])\n",
    "print()\n",
    "\n",
    "#combines the two ideas to isolate specific segments of T\n",
    "print('This is the 0,2,0:2 slice of T:')\n",
    "print(T[0,2,0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the extended slicing functionality, which supports negative indices and a second colon for the `[start:stop:step]` syntax. Try playing around with slices of a many-dimensional NumPy array. Before you run the code, can you guess what shape the slice will have and which elements it will contain? Which slice of the above tensor `T` will give the array `[[11 12] [26 27]]`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Image Manipulation 1\n",
    "\n",
    "Use aggregating functions to manipulate the images in the `images.npy` file. This cell will save these manipulated images in the local folder. Look at the images to check your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "    \n",
    "'''\n",
    "Converts a series of RGB images to a series of grayscale images\n",
    "\n",
    "@param array an (i x c x h x w) numpy array\n",
    "       i is the image axis\n",
    "       c is the color axis\n",
    "       h is the height axis\n",
    "       w is the width axis\n",
    "@return an (i x h x w) array of grayscale images\n",
    "'''\n",
    "\n",
    "def grayscaleImages(array):\n",
    "    pass\n",
    "\n",
    "    \n",
    "'''\n",
    "Converts a series of RGB images to a series of luminosity-map images based on given weights\n",
    "\n",
    "@param array an (i x c x h x w) numpy array\n",
    "       i is the image axis\n",
    "       c is the color axis\n",
    "       h is the height axis\n",
    "       w is the width axis\n",
    "@param r the weight the first channel should have on the luminosity\n",
    "@param g the weight the second channel should have on the luminosity\n",
    "@param b the weight the third channel should have on the luminosity\n",
    "@return a (i x h x w) array whose values are the weighted average of the red, green, and blue channel values\n",
    "'''\n",
    "def luminosityImages(array, r, g, b):\n",
    "    pass\n",
    "\n",
    "images = np.load('images.npy')\n",
    "grayscales = grayscaleImages(images)\n",
    "Image.fromarray(grayscales[0].astype(np.uint8), 'L').save('pic1Grayscale.jpg')\n",
    "Image.fromarray(grayscales[1].astype(np.uint8), 'L').save('pic2Grayscale.jpg')\n",
    "Image.fromarray(grayscales[2].astype(np.uint8), 'L').save('pic3Grayscale.jpg')\n",
    "luminosities = luminosityImages(images,1,1,1)\n",
    "Image.fromarray(luminosities[0].astype(np.uint8), 'L').save('pic1Luminosity.jpg')\n",
    "Image.fromarray(luminosities[1].astype(np.uint8), 'L').save('pic2Luminosity.jpg')\n",
    "Image.fromarray(luminosities[2].astype(np.uint8), 'L').save('pic3Luminosity.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Aggregate Functions\n",
    "\n",
    "So far we've talked a lot about tensors and their shapes, but one term we should define is an **axis**. A *tensor of order $n$* has $n$ axes. You can think of a single axis as \"direction\" along which you can traverse the tensor. Since a vector is a *tensor of order 1*, it only has one axis. You can only traverse it vertically (or horizontally. Depends on notion). A matrix has two axes. You can traverse it horizontally or vertically. In practice, different axes store different *types* of information. \n",
    "\n",
    "For example, if we have a single 360x720 image with RGB values for each pixel, then it would be stored as a *tensor of order 3* with a shape of (3,360,720) representing (#color channels, height, width). Abstractly, it doesn't matter what order the axes are in, i.e. (720,3,360) is completely valid, but in practice there are often standards for what axes represent what information. The most important thing is making sure you stay consistent with whatever standards you adopt. Here we're using PyTorch's \"channels first\" standard of putting the number of (color) channels first. Now, if we wanted to, we could also represent the image in this shape: (1,3,1,360,1,1,1,720). Would that be useful? Probably not. With that being said, sometimes it is quite helpful to include an extra axis in machine learning applications, but that'll be covered later.\n",
    "\n",
    "One of the best features of numpy is their incredibly well optimized system of aggregate functions. These functions also make tensors much more useful. An aggregate function is one that *aggregates*, or *reduces*, a certain value along certain axes. For example, perhaps the most common aggregate function is the `np.sum` function. By default, most aggregate functions will reduce along *all* axes, and return a single scalar, but you can specify along which axes they reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes a 3x2x2 tensor T with random values from 0 to 10\n",
    "T = np.random.random([3,2,2]) * 10\n",
    "print('This is a tensor T:')\n",
    "print(T)\n",
    "print()\n",
    "\n",
    "#sums the elements of T\n",
    "print('This is the sum of the elements of T:')\n",
    "print(np.sum(T))\n",
    "print()\n",
    "\n",
    "#sums the elements of T along the first axis\n",
    "print('This is the sum of the elements of T along the first axis:')\n",
    "print(np.sum(T, axis=0))\n",
    "print()\n",
    "\n",
    "#sums the elements of T along the first axis with a for-loop\n",
    "U = np.zeros([T.shape[1], T.shape[2]])\n",
    "for i in range(T.shape[0]):\n",
    "    for j in range(T.shape[1]):\n",
    "        for k in range(T.shape[2]):\n",
    "            U[j,k] += T[i,j,k]\n",
    "print('This is the sum of the elements of T along the first axis given by a for-loop:')\n",
    "print(U)\n",
    "print()\n",
    "\n",
    "#sums the elements of T along the second and third axes simultaneously\n",
    "print('This is the sum of the elements of T along the second and third axes:')\n",
    "print(np.sum(T, axis=(1,2)))\n",
    "print()\n",
    "\n",
    "#sums the elements of T along the second and third axes\n",
    "V = np.zeros(T.shape[0])\n",
    "for i in range(T.shape[0]):\n",
    "    for j in range(T.shape[1]):\n",
    "        for k in range(T.shape[2]):\n",
    "            V[i] += T[i,j,k]\n",
    "print('This is the sum of the elements of T along the second and third axes given by a for-loop:')\n",
    "print(V)\n",
    "print()\n",
    "\n",
    "#multiplies the elements of T together\n",
    "print('This is the product of the elements of T:')\n",
    "print(np.prod(T))\n",
    "print()\n",
    "\n",
    "#multiplies the elements of T along the third axis\n",
    "#this produces a 3x2 matrix consisting of the columns of each matrix of T element-wise multiplied together\n",
    "print('This is the product of the elements of T along the third axis:')\n",
    "print(np.prod(T, axis=2))\n",
    "print()\n",
    "\n",
    "#finds the maximum and minimum elements of T along the 2nd axis\n",
    "#produces 3x2 matrices consisting of the maximal/minimal element of each column of the matrices composing T\n",
    "print('This is maximum reduction of T along the second axis:')\n",
    "print(np.max(T, axis=1))\n",
    "print('This is the minimum reduction of T along the second axis:')\n",
    "print(np.min(T, axis=1))\n",
    "print()\n",
    "\n",
    "#finds the average and median of the elements of T along the first axis\n",
    "print('This is the average reduction of T along the first axis:')\n",
    "print(np.mean(T, axis=0))\n",
    "print('This is the median reduction of T along the first axis:')\n",
    "print(np.median(T, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Image Manipulation 2\n",
    "\n",
    "Use aggregating functions to manipulate the images in the `images.npy` file. This cell will save these manipulated images in the local folder. Look at the images to check your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "'''\n",
    "Constructs an RGB image whose values are the average of a series of images\n",
    "\n",
    "@param array an (i x c x h x w) numpy array\n",
    "       i is the image axis\n",
    "       c is the color axis\n",
    "       h is the height axis\n",
    "       w is the width axis\n",
    "@return a (c x h x w) array whose values are the average RGB values of the images in array\n",
    "'''\n",
    "def averageImage(array):\n",
    "    pass\n",
    "    \n",
    "    \n",
    "    \n",
    "'''\n",
    "Converts a series of RGB images to a series of grayscale images\n",
    "\n",
    "@param array an (i x c x h x w) numpy array\n",
    "       i is the image axis\n",
    "       c is the color axis\n",
    "       h is the height axis\n",
    "       w is the width axis\n",
    "@return an (i x h x w) array of grayscale images\n",
    "'''\n",
    "def grayscaleImages(array):\n",
    "    pass\n",
    "\n",
    "images = np.load('images.npy')\n",
    "Image.fromarray(np.moveaxis(averageImage(images).astype(np.uint8),0,2)).save('avg.jpg')\n",
    "grayscales = grayscaleImages(images)\n",
    "Image.fromarray(grayscales[0].astype(np.uint8), 'L').save('pic1Grayscale.jpg')\n",
    "Image.fromarray(grayscales[1].astype(np.uint8), 'L').save('pic2Grayscale.jpg')\n",
    "Image.fromarray(grayscales[2].astype(np.uint8), 'L').save('pic3Grayscale.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiling\n",
    "\n",
    "Tiling is a technique used when creating or modifying tensors which essentially repeats tensors along certain axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates a vector of length 6 with values from 0 to 5\n",
    "v = np.linspace(0,5,6)\n",
    "print('This is a vector v of length 6 with values from 0 to 5:')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#constructs a vector length 18 by tiling v\n",
    "print('This is a vector of length 18 constructed by tiling v 3 times along its axis')\n",
    "print(np.tile(v, 3))\n",
    "print()\n",
    "\n",
    "#constructs a 6x6 matrix by tiling v\n",
    "print('This is a 6x6 matrix constructed by tiling v 6 times along a new first axis:')\n",
    "print(np.tile(v, [5,1]))\n",
    "print()\n",
    "\n",
    "#constructs a 2x4x6 tensor by tiling v\n",
    "print('This is a 2x4x6 tensor constructed by tiling v 2 times along a new first axis and 4 times along a new second axis:')\n",
    "print(np.tile(v, [2,4,1]))\n",
    "print()\n",
    "\n",
    "#initializes a 6x6 matrix with values from 0 to 10\n",
    "A = np.stack([np.linspace(i,i+5,6) for i in range(0,6,1)], axis=1)\n",
    "print('This is a 6x6 matrix with values from 0 to 10:')\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "#constructs a 2x2x6x6 tensor with values formed by tiling A\n",
    "print('This is a 2x2x6x6 tensor constructed by tiling A 2 times along two new axes:')\n",
    "print(np.tile(A, [2,2,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 - Norms and Metrics\n",
    "\n",
    "### Metrics\n",
    "\n",
    "A **metric** is a general way to measure the distance between two things. You already have some experience with metrics, whether you know it by that name or not. How close are $3,7$? Well they're only $4$ units away. What about $-3,-7$? $4$ as well. Same as $7,3$. When dealing with real numbers, the metric we usually use is  $d(x,y)=|x-y|$, which is probably the metric you would intuitively use to define distance. There's nothing stopping us from having a metric on things other than real numbers, for example, vectors! The most classic example is probably *euclidean distance*, defined as the square root of the sum of squares. To define our *euclidean distance* metric $d: \\mathbb{R}^n\\times\\mathbb{R}^n\\rightarrow [0,\\infty)$, we say that\n",
    "\n",
    "$$d(x,y)=\\sqrt{\\sum_{i=1}^n(x_i-y_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes 2 vectors, u and v, of length 3\n",
    "u = np.array([1,0,1])\n",
    "v = np.array([-1,0,1])\n",
    "print('This is a vector u:')\n",
    "print(u)\n",
    "print()\n",
    "print('This is a vector v:')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "def euclideanDistance(a,b, debug=False):\n",
    "    difference = a - b\n",
    "    squared_difference = difference * difference\n",
    "    sum_of_squares = np.sum(squared_difference)\n",
    "    distance = np.sqrt(sum_of_squares)\n",
    "    if debug:\n",
    "        print('The difference between a and b is ', difference)\n",
    "        print('Element-wise squaring gets us  ', squared_difference)\n",
    "        print('The sum of those squares is ', sum_of_squares)\n",
    "        print('The square root of the sum is ', distance)\n",
    "        print('Therefore the euclidean distance is: ')\n",
    "    return distance\n",
    "\n",
    "#finds the euclidean distance between u and v\n",
    "print('This is the euclidean distance between u and v:')\n",
    "print(euclideanDistance(u, v, debug=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a formal, mathematical sense, a metric on a space $X$ is a function $d: X \\times X \\rightarrow [0,\\infty)$ that obeys 4 properties, where $x, y, z$ are elements of $X$:\n",
    "\n",
    "- Nonnegativity, $d(x,y) \\geq 0$\n",
    "- Identity of indiscernables, $d(x,y) = 0 \\iff x = y$\n",
    "- Symmetry, $d(x,y)=d(y,x)$\n",
    "- Triangle inequality, $d(x,z) \\leq d(x,y) + d(y,z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms\n",
    "\n",
    "A similar notion is a **norm**, which gives a general way to think about the size of an object. You can think of it as a generalization of the absolute value function. In fact, $f(x)=|x|$ is a proper norm. Given a metric $d$, we can define a norm $f(x)=d(0,x)$, or its distance from zero (or whatever represents \"zero\", e.g. the zero vector). We then say that this is the norm *induced* by our given metric. Similarly, you can generally induce a metric from a norm by defining $d(x,y)=f(x-y)$, so we'll usually discuss these two semi-interchangeably. When you've already established a norm that you're working in, or it is otherwise obvious, then instead of referring to the actual norm function, you can instead refer to the double-bar notation: $f(x)=\\|x\\|$. \n",
    "\n",
    "Norms and metrics are incredibly important in data science, and machine learning especially, since they define the idea of **distance**. That notion of **distance** also defines how similar/dissimilar data is to other data. This results in determining the shape, or *topology*, of the space that your data exists in. It's all very abstract right now, but we'll see concrete examples of this in later workshops. \n",
    "\n",
    "A special family of norms exists, known as $L^p$ norms $L^p:\\mathbb{R}^n\\rightarrow[0,\\infty)$, defined by: $$L^p(x)=(\\sum_{i=1}^n(|x_i|)^p)^{1/p}$$This is a widely used family, including two very heavily used norms: $$L^1(x)=\\sum_{i=1}^n|x_i|, \\quad\\text{and}\\quad L^2(x)=(\\sum_{i=1}^n|x_i|^2)^{1/2}=\\sqrt{\\sum_{i=1}^nx_i^2}$$\n",
    "\n",
    "$L^2$ is the familiar euclidean norm, while $L^1$ is a norm popularly used in image analysis. Another nice property is that in $\\mathbb{R}^n$,\n",
    "$$L^2(x) = \\sqrt{x\\cdot x} = \\sqrt{x^\\intercal x}.$$\n",
    "There are many ways to write and compute the same idea! Let's check out what these things look like in NumPy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes two vectors, u and v, of length 4\n",
    "u = np.array([1,2,3,4])\n",
    "v = np.array([-1,4,-3,5])\n",
    "print('This is a vector u:')\n",
    "print(u)\n",
    "print()\n",
    "print('This is a vector v:')\n",
    "print(v)\n",
    "print()\n",
    "\n",
    "#finds the L1 norm of v\n",
    "print('This is the L1 norm of v:')\n",
    "print(np.linalg.norm(v,1))\n",
    "print()\n",
    "\n",
    "#finds the L2 norm of v\n",
    "print('This is the L2 norm of v:')\n",
    "print(np.linalg.norm(v,2))\n",
    "print()\n",
    "\n",
    "#shows the relationship between L2 norm and euclidean distance\n",
    "print('Note that the L2 norm of u-v is:')\n",
    "print(np.linalg.norm(u-v,2))\n",
    "print('while the euclidean distance between u and v is:')\n",
    "print(euclideanDistance(u,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - $L^p$ Norms\n",
    "\n",
    "The $L^p$ family of norms is already implemented as the function `norm` in the `numpy.linalg` module, but we can also implement it ourselves in a straightforward way. Note that you can use the standard Python exponent operator `**` to perform element-wise exponentiation on a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_p_norm(x, p=2):\n",
    "    '''Calculates the L^p norm of a vector\n",
    "    \n",
    "    x: a vector\n",
    "    p: the parameter for the norm\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- TESTS --------\n",
    "\n",
    "v = np.random.randn(10)\n",
    "\n",
    "# should be equal (to within a rounding error)\n",
    "# L^2 norm\n",
    "print(l_p_norm(v))\n",
    "print(np.linalg.norm(v, 2))\n",
    "\n",
    "# L^5 norm\n",
    "print(l_p_norm(v, p=5))\n",
    "print(np.linalg.norm(v, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "The practical implications of norms and metrics come in the form of so called **loss functions**. In data science, a **loss function** is a function which **your model attempts to optimize**. In linear regression, it is MSE, in classification it could be cross-entropy, in variational auto encoders, it is a lower bound on marginal probability. Loss functions are *deeply* context based, and can sometimes freely vary even within a given problem. Loss functions are neither metrics nor norms, but something else entirely. They don't have any limitations, which means unlike metrics, they don't have to be symmetric, or even take two inputs. Some loss functions are notoriously never zero (but approach arbitrarily close to zero). They just have to represent *some* function you want to optimize. This means that there can be *many* more loss functions in practice than just metrics or norms. There are a plethora of loss functions out there, and most of them will be handled in detail during future workshops. \n",
    "\n",
    "With that being said, we've already covered perhaps the most widespread family of loss functions: the $L^p$ norms and their induced metrics! Even the familiar Mean Squared Error (MSE) loss function in $\\mathbb{R}^n$ is $\\frac{1}{n}\\sum_{i=1}^n(x_i-y_i)^2=\\frac{1}{n}(L^2(\\vec x-\\vec y))^2=\\frac{1}{n}(\\vec x-\\vec y)\\cdot(\\vec x-\\vec y)$. It's not *immediately* the same as the $L^2$ norm, but that formula isn't a coincidence either. The squaring and averaging by $n$ are just mechanisms employed to make the function more convenient. For example, squaring it makes taking the derivative *much* easier. MSE as a loss function also has its roots in statistical theory, like many other loss functions. MSE can be considered the *expected value* of the square of the $L^2$ norm. This interpretation is especially helpful when the MSE can't be calculated over *all* the data and hence must be *sampled*. That exists somewhat out of the scope of this workshop, but we encourage you to explore these topics since they're a recurring theme in data science! Similarly, Mean Absolute Error (MAE), another commonly used loss function is $\\frac{1}{n}\\sum_{i=1}^n|x_i-y_i|=\\frac{1}{n}L^1(\\vec x-\\vec y)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Dimensionality Reduction\n",
    "\n",
    "\n",
    "### Projecting Onto a Vector\n",
    "This is a huge topic in data science, with many dedicated sub-fields, so we won't be able to cover everything, but it's worth understanding the principles behind. The idea of dimensionality reduction comes from a very practical use: compression. Not just image compression, but also for arbitrary files, binary strings, and all sorts of data! Compression has the obvious benefit of *making our relevant data smaller*, but it also tends to extract the most relevant and meaningful information from our original data set. I say \"tends to\" since not all compression strategies accomplish this. One of the (mathematically) simplest forms of dimensionality reduction is projection! Let's go back to vectors for a second. Say we have vectors $\\vec u, \\vec v\\in\\mathbb{R}^n$, then we can *project u onto v*, getting a new vector $\\operatorname{proj}_{\\vec v}\\vec u$. In reality, we're projecting $\\vec u$ onto the subspace generated by $\\vec v$, known as $\\operatorname{span}\\{\\vec v\\}$. which is just all scalar multiples of $\\vec v$, which forms a single infinite line in $\\mathbb{R}^n$.  \n",
    "\n",
    "**Remark**: vectors $\\vec v, \\vec u$ are *parallel* if $\\exists c\\in\\mathbb{R}$ so that $\\vec u=c\\vec v$. We then say that they are *perpendicular or orthogonal* if $\\vec v \\cdot\\vec u =0$\n",
    "\n",
    "So before we go into the math of how projections work, what does a projection look like? If you have vectors $\\vec u, \\vec v\\in\\mathbb{R}^n$ then $\\vec w=\\operatorname{proj}_{\\vec v}\\vec u$ is the *component of $\\vec u$ in the direction of $\\vec v$*. ![Image of Yaktocat](http://blogs.jccc.edu/rgrondahl/files/2012/02/parallelprojection1.jpg)\n",
    "This tells us that $\\vec w=c\\vec v$ for some $c\\in\\mathbb{R}$, that is to say, the projection will be *parallel* to the vector $\\vec v$. So know we know its direction, and need only figure out its length. Since $w=c\\vec u$ is the component of $\\vec u$ in the direction of $\\vec v$, we say that $\\vec u - \\vec w$ is then orthogonal to $\\vec v$ since it no longer has any component in the direction of $\\vec v$, so $$\\vec v \\cdot (\\vec u - \\vec w)=0\\implies \\vec v \\cdot \\vec u - \\vec v \\cdot \\vec w=0\\implies\\vec v \\cdot \\vec u = \\vec v \\cdot \\vec w =\\vec v \\cdot c\\vec v= c(\\vec v \\cdot \\vec v)=c \\|\\vec v\\|^2\\implies\\frac{\\vec v \\cdot \\vec u}{\\|\\vec v\\|^2}$$. Putting that all together, we get $$\\vec w= \\frac{\\vec v \\cdot \\vec u}{\\|\\vec v\\|^2}\\vec v$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting Onto a Plane\n",
    "\n",
    "So for now, let's fix our space as $\\mathbb{R}^3$ for now and focus on projecting onto a plane. This is going to be just a regular plane with *one* added condition: it contains the origin. If we were to try to project $\\vec u$ onto a plane $S$, then what we want is another vector $\\vec w$ that is the *component of $\\vec u$ in the direction of $S$*. So far the goal is identical to last time where we projected onto a vector, but the strategy we used last time won't *exactly* work here. The first thing we did was to find the direction of $\\vec w$, but can we really do that right now? It was easy last time since there was only one direction it could be in, since the line was generated by a single vector $\\vec v$, but that's not the case anymore. Here we can leverage a certain fact though: for this plane, since it contains $\\vec 0$, it turns out that there are two vectors, $\\vec v_1, \\vec v_2$ that are a *basis* for the plane. That is to say, $x\\in S\\iff \\exists c_1,c_2\\in \\mathbb{R}$ such that $\\vec x=c_1\\vec v_1+c_2\\vec v_2$, or equivelantly $S=\\operatorname{span}\\{\\vec v_1, \\vec v_2\\}$. Now let's also assume that $\\vec v_1,\\vec v_2$ are orthogonal, or $\\vec v_1 \\cdot \\vec v_2=0$. Then it turns out that $$\\operatorname{proj}_S \\vec u =\\operatorname{proj}_{\\vec v_1} \\vec u+\\operatorname{proj}_{\\vec v_2} \\vec u = \\frac{\\vec v_1 \\cdot \\vec u}{\\|\\vec v_1\\|^2}\\vec v_1+\\frac{\\vec v_2 \\cdot \\vec u}{\\|\\vec v_2\\|^2}\\vec v_2$$ But why is that the case? Well, even though the direction of $\\vec w$ isn't immediately obvious, since ${\\vec v_1,\\vec v_2}$ form a basis, we know that $\\vec u=c_1\\vec v_1 + c_2 \\vec v_2$ for *some* values of $c_1, c_2$, then we only need to figure out those two values! We can figure out $c_1,c_2$ just like we did for $c$ in the last problem. $$\\vec v_1 \\cdot \\vec u = \\vec v_1 \\cdot (c_1\\vec v_1 + c_2 \\vec v_2)=c_1(\\vec v_1 \\cdot \\vec v_1) + c_2( \\vec v_2\\cdot \\vec v_1)=c_1(\\vec v_1 \\cdot \\vec v_1) + c_2(0)=c_1\\|\\vec v_1 \\|^2\\implies c_1=\\frac{\\vec v_1 \\cdot \\vec u}{\\|\\vec v_1\\|^2}$$\n",
    "\n",
    "And the *exact same* process can be applied to find $c_2$\n",
    "\n",
    "### Projecting Onto a Subspace\n",
    "\n",
    "\n",
    "In the last section, we made a couple of assumptions. These assumptions made the problem *much* easier by ensuring that $S$ was a **subspace**. A subspace $S$ of $\\mathbb{R}^n$ is a region (subset) of $\\mathbb{R}^n$, written as $S\\subset\\mathbb{R}^n$ which obeys these properties:\n",
    "\n",
    "1) It contains the zero vector\n",
    "$$\\vec 0\\in S$$\n",
    "\n",
    "2) It is closed under addition\n",
    "$$\\vec x, \\vec y\\in S \\implies \\vec x + \\vec y \\in S$$\n",
    "\n",
    "3) It is closed under scalar multiplication\n",
    "$$c\\in \\mathbb{R}, \\vec x\\in S\\implies c\\vec x\\in S$$\n",
    "\n",
    "You should go back and check that the way generated $S$ follows these rules. Once you know that you're working with a subspace, you can find vectors ${\\vec v_1,...,\\vec v_k}$ such that $S=\\operatorname{span}\\{\\vec v_1,...,\\vec v_k\\}$. Then, in general, $$\\operatorname{proj}_S \\vec u= \\sum_{i=1}^k \\operatorname{proj}_{\\vec v_i} \\vec u=\\sum_{i=1}^k \\frac{\\vec v_i \\cdot \\vec u}{\\|\\vec v_i\\|^2}\\vec v_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Projection onto a vector\n",
    "\n",
    "Implement a function that projects a vector onto another vector of the same dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj(v, direction):\n",
    "    '''Returns the projection of a vector onto another\n",
    "    \n",
    "    v: the vector to project\n",
    "    direction: the vector to be projected onto\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
